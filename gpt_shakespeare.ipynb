{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 110157,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 92280,
          "modelId": 116481
        }
      ],
      "dockerImageVersionId": 30762,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!wget \"https://raw.githubusercontent.com/romenlaw/NaiveNeuralNetwork/main/tinysp_vocab_min.json\"\n",
        "!wget \"https://raw.githubusercontent.com/romenlaw/NaiveNeuralNetwork/main/tinysp_vocab.json\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1MF8XY4lVgb",
        "outputId": "2ff087d5-b3a4-41b3-8b65-ce2b7f457b95",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:28.389261Z",
          "iopub.execute_input": "2024-09-09T06:42:28.389748Z",
          "iopub.status.idle": "2024-09-09T06:42:42.541671Z",
          "shell.execute_reply.started": "2024-09-09T06:42:28.389711Z",
          "shell.execute_reply": "2024-09-09T06:42:42.540475Z"
        },
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: 1: command not found\n",
            "--2024-10-13 03:29:03--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-10-13 03:29:03 (17.4 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n",
            "/bin/bash: line 1: 1: command not found\n",
            "--2024-10-13 03:29:03--  https://raw.githubusercontent.com/romenlaw/NaiveNeuralNetwork/main/tinysp_vocab_min.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 124965 (122K) [text/plain]\n",
            "Saving to: ‘tinysp_vocab_min.json.1’\n",
            "\n",
            "tinysp_vocab_min.js 100%[===================>] 122.04K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-10-13 03:29:03 (4.54 MB/s) - ‘tinysp_vocab_min.json.1’ saved [124965/124965]\n",
            "\n",
            "/bin/bash: line 1: 1: command not found\n",
            "--2024-10-13 03:29:03--  https://raw.githubusercontent.com/romenlaw/NaiveNeuralNetwork/main/tinysp_vocab.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 147427 (144K) [text/plain]\n",
            "Saving to: ‘tinysp_vocab.json.1’\n",
            "\n",
            "tinysp_vocab.json.1 100%[===================>] 143.97K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-10-13 03:29:03 (4.23 MB/s) - ‘tinysp_vocab.json.1’ saved [147427/147427]\n",
            "\n"
          ]
        }
      ],
      "id": "u1MF8XY4lVgb"
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", \"r\", encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoBdNCJMglmf",
        "outputId": "25e45865-1498-4aee-fc76-f00fa6180ab6",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:44.207843Z",
          "iopub.execute_input": "2024-09-09T06:42:44.208386Z",
          "iopub.status.idle": "2024-09-09T06:42:44.217817Z",
          "shell.execute_reply.started": "2024-09-09T06:42:44.208336Z",
          "shell.execute_reply": "2024-09-09T06:42:44.216860Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "id": "EoBdNCJMglmf"
    },
    {
      "cell_type": "code",
      "source": [
        "text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "3IXxpL3Gg-n9",
        "outputId": "51be1f19-7e0c-4ee2-8a6b-35d35133fb2c",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:44.220297Z",
          "iopub.execute_input": "2024-09-09T06:42:44.220635Z",
          "iopub.status.idle": "2024-09-09T06:42:44.256980Z",
          "shell.execute_reply.started": "2024-09-09T06:42:44.220603Z",
          "shell.execute_reply": "2024-09-09T06:42:44.255915Z"
        },
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "id": "3IXxpL3Gg-n9"
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xc2SJtyhRK1",
        "outputId": "e1d459d5-7a77-4bed-93d6-3703c8aae9e1",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:44.258179Z",
          "iopub.execute_input": "2024-09-09T06:42:44.258490Z",
          "iopub.status.idle": "2024-09-09T06:42:44.263131Z",
          "shell.execute_reply.started": "2024-09-09T06:42:44.258457Z",
          "shell.execute_reply": "2024-09-09T06:42:44.262270Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "id": "1Xc2SJtyhRK1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokeniser\n",
        "\n",
        "The tokeniser vocab is created in the [tokenisation notebook](https://github.com/romenlaw/NaiveNeuralNetwork/blob/main/tokenisation.ipynb) using tiny Shakespeare as input data.\n",
        "\n",
        "There are two versions of the vocab:\n",
        "* tinysp_vocab.json - created with byte pair algorithm using tiny Shakepeare as input. There are 10237 tokens.\n",
        "* tinysp_vocab_min.json - same as above except that unused tokens (in tiny Shakespeare) has been removed (hence, the token integers are not contiguous). There are 8652 tokens."
      ],
      "metadata": {
        "id": "3oTmqoV2kK40"
      },
      "id": "3oTmqoV2kK40"
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "with open(\"tinysp_vocab.json\", \"r\") as f:\n",
        "  input = f.read()\n",
        "vocab = ast.literal_eval(input)\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "qufe8uOwlQGz",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:44.264248Z",
          "iopub.execute_input": "2024-09-09T06:42:44.264583Z",
          "iopub.status.idle": "2024-09-09T06:42:49.408361Z",
          "shell.execute_reply.started": "2024-09-09T06:42:44.264551Z",
          "shell.execute_reply": "2024-09-09T06:42:49.407361Z"
        },
        "trusted": true
      },
      "execution_count": 49,
      "outputs": [],
      "id": "qufe8uOwlQGz"
    },
    {
      "cell_type": "code",
      "source": [
        "def count_bytepairs(bytes):\n",
        "  counts = {}\n",
        "  for pair in zip(bytes, bytes[1:]):\n",
        "    counts[pair] = counts.get(pair, 0) + 1\n",
        "  counts = dict(sorted(counts.items(), key=lambda item: item[1], reverse=True))\n",
        "  return counts\n",
        "\n",
        "def merge_bytepair(raw_bytes, pair, new_token):\n",
        "  \"\"\"pair is a list containing two integers, representing the utf-8 code of\n",
        "  the 2 characters to be replaced\n",
        "  \"\"\"\n",
        "  assert isinstance(pair, list) and len(pair)==2, \"pair must be a list of two integers\"\n",
        "  for i in range(len(raw_bytes)-1, 0, -1):  # loop backwards\n",
        "    #print(i-1, i)\n",
        "    if raw_bytes[i-1:i+1] == pair:\n",
        "      raw_bytes[i-1:i+1] = [new_token]\n",
        "\n",
        "  return raw_bytes\n",
        "\n",
        "num_merges = vocab_size - 256\n",
        "merges = {} # build up a lookup of what has been merged into what\n",
        "\n",
        "def encode(s, tokens=None):\n",
        "  \"\"\"Given a string, return a list of integers using utf-8 and byte pair\n",
        "  tokens is a list of utf-8 codes or encoded byte-pair codes, updated in recursion\n",
        "  \"\"\"\n",
        "  assert isinstance(s, str), f\"expect string, but got {type(s).__name__}\"\n",
        "  tokens = list(s.encode(\"utf-8\")) if tokens is None else tokens\n",
        "  counts = count_bytepairs(tokens)\n",
        "  # if len is too small, next(counts_iter) in below loop will fail\n",
        "  # if no more duplicated byte pairs, then stop the recursion\n",
        "  if len(counts) <=1 or counts[list(counts)[0]]<=1:\n",
        "      return tokens\n",
        "\n",
        "  new_token = max(tokens+[255])\n",
        "  counts_iter = iter(counts.items())\n",
        "  global num_merges\n",
        "  #for i in range(num_merges):\n",
        "  while num_merges>0:\n",
        "    num_merges-=1\n",
        "    new_token += 1\n",
        "    pair, count = next(counts_iter)\n",
        "    if count>1:\n",
        "      pair_copy = list(pair)\n",
        "      merge_bytepair(tokens, pair_copy, new_token)\n",
        "      merges[pair] = new_token\n",
        "    else:\n",
        "      # if the current duplicated byte pairs are exhausted, refresh the counts\n",
        "      # and continue to encode\n",
        "      encode(s, tokens)\n",
        "      break\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "yhwX-NNbD1H3"
      },
      "id": "yhwX-NNbD1H3",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(ids):\n",
        "  \"\"\"Given a list of tokens (integers), return a python string\n",
        "  \"\"\"\n",
        "  assert isinstance(ids, list), \"ids must be list\"\n",
        "  assert all((isinstance(i, int) and i>=0 ) for i in ids), \\\n",
        "    \"ids must be a list of integers in [0, vocab_size)\"\n",
        "#   out = bytes()\n",
        "#   for b in ids:\n",
        "#     out += vocab[b]\n",
        "  out = b\"\".join(vocab[b] for b in ids)\n",
        "  return out.decode(\"utf-8\", errors='replace') # see utf-8 spec"
      ],
      "metadata": {
        "id": "WGp_Emy69923"
      },
      "id": "WGp_Emy69923",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"hello world with this!\"))"
      ],
      "metadata": {
        "id": "o4gNRUAFiGN1",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:49.467169Z",
          "iopub.execute_input": "2024-09-09T06:42:49.467798Z",
          "iopub.status.idle": "2024-09-09T06:42:49.474822Z",
          "shell.execute_reply.started": "2024-09-09T06:42:49.467765Z",
          "shell.execute_reply": "2024-09-09T06:42:49.473924Z"
        },
        "trusted": true,
        "outputId": "feceb367-3bf4-40e7-e505-10460455a2dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[104, 101, 108, 108, 111, 256, 111, 114, 108, 100, 256, 105, 257, 32, 257, 105, 115, 33]\n"
          ]
        }
      ],
      "id": "o4gNRUAFiGN1"
    },
    {
      "cell_type": "code",
      "source": [
        "decode([104, 101, 108, 108, 111, 256])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IPvVK-ntaic-",
        "outputId": "a624cd97-da2c-433f-a44c-7b6f2dc01331",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:49.476007Z",
          "iopub.execute_input": "2024-09-09T06:42:49.476622Z",
          "iopub.status.idle": "2024-09-09T06:42:49.487041Z",
          "shell.execute_reply.started": "2024-09-09T06:42:49.476580Z",
          "shell.execute_reply": "2024-09-09T06:42:49.486229Z"
        },
        "trusted": true
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'helloe '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "id": "IPvVK-ntaic-"
    },
    {
      "cell_type": "code",
      "source": [
        "decode(encode(text[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jExItzdEiSFN",
        "outputId": "fb1ac50f-57ce-4a43-9f72-0386757310ca",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:49.488339Z",
          "iopub.execute_input": "2024-09-09T06:42:49.488710Z",
          "iopub.status.idle": "2024-09-09T06:42:49.498256Z",
          "shell.execute_reply.started": "2024-09-09T06:42:49.488669Z",
          "shell.execute_reply": "2024-09-09T06:42:49.497365Z"
        },
        "trusted": true
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'onBeforthwthproceed any furtanrrehe r mthesAll tSpe kreesonYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "id": "jExItzdEiSFN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "HQ_F0ce2oddD"
      },
      "id": "HQ_F0ce2oddD"
    },
    {
      "cell_type": "code",
      "source": [
        "# let's encode our dataset\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "wshkuMvpmwRN",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:49.499325Z",
          "iopub.execute_input": "2024-09-09T06:42:49.499630Z",
          "iopub.status.idle": "2024-09-09T06:42:49.772472Z",
          "shell.execute_reply.started": "2024-09-09T06:42:49.499599Z",
          "shell.execute_reply": "2024-09-09T06:42:49.771465Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "wshkuMvpmwRN"
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape, data.dtype, data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU7XH-Hwm6bn",
        "outputId": "102ad875-b113-4913-a662-28b0c19d09bc",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:49.773841Z",
          "iopub.execute_input": "2024-09-09T06:42:49.774254Z",
          "iopub.status.idle": "2024-09-09T06:42:49.815093Z",
          "shell.execute_reply.started": "2024-09-09T06:42:49.774199Z",
          "shell.execute_reply": "2024-09-09T06:42:49.814240Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([1115394]),\n torch.int64,\n tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n         53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n          1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n         57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n          6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n         58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n          1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n         53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n         57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n          8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n          1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n         53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n         47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n          1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n         50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n         49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n         47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n         46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n         43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n         54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n         47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n          1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n          1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n          1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n         47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n         53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n         58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n         39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n         47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n         39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n         46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n          1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n          1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n         50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n         56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n         61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n          1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n         56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n         50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n          1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n         58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n         39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n         40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n         63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n         53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n         57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n         11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n         57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n         43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n          1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n         56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n         10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n         61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n         46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n         52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n         56, 43, 60, 43, 52, 45, 43,  8,  0,  0]))"
          },
          "metadata": {}
        }
      ],
      "id": "MU7XH-Hwm6bn"
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset into training (90%) and validation (10%)\n",
        "n = int(data.shape[0] * .9)\n",
        "data_train = data[:n]\n",
        "data_val = data[n:]"
      ],
      "metadata": {
        "id": "lmum-u7jnT7f",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:49.816329Z",
          "iopub.execute_input": "2024-09-09T06:42:49.816712Z",
          "iopub.status.idle": "2024-09-09T06:42:49.821704Z",
          "shell.execute_reply.started": "2024-09-09T06:42:49.816670Z",
          "shell.execute_reply": "2024-09-09T06:42:49.820788Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "lmum-u7jnT7f"
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.shape, data_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQeAUIzJnnyD",
        "outputId": "9e82ed10-e711-4385-c50f-6053e7f4de19",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:49.822955Z",
          "iopub.execute_input": "2024-09-09T06:42:49.823314Z",
          "iopub.status.idle": "2024-09-09T06:42:49.833214Z",
          "shell.execute_reply.started": "2024-09-09T06:42:49.823274Z",
          "shell.execute_reply": "2024-09-09T06:42:49.832433Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([1003854]), torch.Size([111540]))"
          },
          "metadata": {}
        }
      ],
      "id": "oQeAUIzJnnyD"
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8 # context/chunk length\n",
        "data_train[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VBiTEPzoFXc",
        "outputId": "90d1329d-dda0-4004-982d-2d25159f3733",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:49.834255Z",
          "iopub.execute_input": "2024-09-09T06:42:49.834559Z",
          "iopub.status.idle": "2024-09-09T06:42:49.844448Z",
          "shell.execute_reply.started": "2024-09-09T06:42:49.834517Z",
          "shell.execute_reply": "2024-09-09T06:42:49.843714Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
          },
          "metadata": {}
        }
      ],
      "id": "3VBiTEPzoFXc"
    },
    {
      "cell_type": "code",
      "source": [
        "# let's have a look at the time-dimension\n",
        "x = data_train[:block_size]\n",
        "y = data_train[1:block_size+1]\n",
        "for i in range(block_size):\n",
        "  context=x[:i+1]\n",
        "  print(f\"from {context}, we want to predict {y[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oUM7vS9onCt",
        "outputId": "8faaa792-c4bc-4337-b2b7-4027931f1458",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:49.845443Z",
          "iopub.execute_input": "2024-09-09T06:42:49.845733Z",
          "iopub.status.idle": "2024-09-09T06:42:49.858694Z",
          "shell.execute_reply.started": "2024-09-09T06:42:49.845704Z",
          "shell.execute_reply": "2024-09-09T06:42:49.857850Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "from tensor([18]), we want to predict 47\nfrom tensor([18, 47]), we want to predict 56\nfrom tensor([18, 47, 56]), we want to predict 57\nfrom tensor([18, 47, 56, 57]), we want to predict 58\nfrom tensor([18, 47, 56, 57, 58]), we want to predict 1\nfrom tensor([18, 47, 56, 57, 58,  1]), we want to predict 15\nfrom tensor([18, 47, 56, 57, 58,  1, 15]), we want to predict 47\nfrom tensor([18, 47, 56, 57, 58,  1, 15, 47]), we want to predict 58\n",
          "output_type": "stream"
        }
      ],
      "id": "0oUM7vS9onCt"
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly generate the starting position of each row of a batch (of size 4)\n",
        "torch.randint(0, len(data_train)-block_size, (4,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kjBZgH8tC_C",
        "outputId": "443295fc-5763-491b-fc6e-a5f105522583",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:49.859751Z",
          "iopub.execute_input": "2024-09-09T06:42:49.860033Z",
          "iopub.status.idle": "2024-09-09T06:42:49.874380Z",
          "shell.execute_reply.started": "2024-09-09T06:42:49.859993Z",
          "shell.execute_reply": "2024-09-09T06:42:49.873351Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "tensor([940401, 296183, 831949,  85911])"
          },
          "metadata": {}
        }
      ],
      "id": "3kjBZgH8tC_C"
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare mini-batch\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch(split='train'):\n",
        "  \"\"\"split is one of 'train', 'val'\n",
        "  \"\"\"\n",
        "  data = data_train if split=='train' else data_val\n",
        "  batch_ix = torch.randint(0, len(data)-block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in batch_ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in batch_ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "UGwoO_Ugqm1M",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:49.875432Z",
          "iopub.execute_input": "2024-09-09T06:42:49.875722Z",
          "iopub.status.idle": "2024-09-09T06:42:49.885914Z",
          "shell.execute_reply.started": "2024-09-09T06:42:49.875692Z",
          "shell.execute_reply": "2024-09-09T06:42:49.885029Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "UGwoO_Ugqm1M"
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')"
      ],
      "metadata": {
        "id": "2UYzsY61veiH",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:49.887070Z",
          "iopub.execute_input": "2024-09-09T06:42:49.887453Z",
          "iopub.status.idle": "2024-09-09T06:42:50.053712Z",
          "shell.execute_reply.started": "2024-09-09T06:42:49.887369Z",
          "shell.execute_reply": "2024-09-09T06:42:50.052691Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "2UYzsY61veiH"
    },
    {
      "cell_type": "code",
      "source": [
        "xb.shape, yb.shape, xb, yb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgMzYu2KvqOP",
        "outputId": "2d7c6c32-8b05-4976-b33c-c6ae9a4cc48b",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:50.054912Z",
          "iopub.execute_input": "2024-09-09T06:42:50.055218Z",
          "iopub.status.idle": "2024-09-09T06:42:50.067199Z",
          "shell.execute_reply.started": "2024-09-09T06:42:50.055187Z",
          "shell.execute_reply": "2024-09-09T06:42:50.066385Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([4, 8]),\n torch.Size([4, 8]),\n tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n         [44, 53, 56,  1, 58, 46, 39, 58],\n         [52, 58,  1, 58, 46, 39, 58,  1],\n         [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0'),\n tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n         [53, 56,  1, 58, 46, 39, 58,  1],\n         [58,  1, 58, 46, 39, 58,  1, 46],\n         [17, 27, 10,  0, 21,  1, 54, 39]], device='cuda:0'))"
          },
          "metadata": {}
        }
      ],
      "id": "FgMzYu2KvqOP"
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(batch_size):\n",
        "  for i in range(block_size):\n",
        "    context=xb[j][:i+1]\n",
        "    print(f\"from {context.tolist()}, we want to predict {yb[j][i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PHdzkPdwFHZ",
        "outputId": "718e1e5e-b86b-4ad2-c378-6da1d929245c",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:50.068374Z",
          "iopub.execute_input": "2024-09-09T06:42:50.068697Z",
          "iopub.status.idle": "2024-09-09T06:42:50.077109Z",
          "shell.execute_reply.started": "2024-09-09T06:42:50.068665Z",
          "shell.execute_reply": "2024-09-09T06:42:50.076091Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "from [24], we want to predict 43\nfrom [24, 43], we want to predict 58\nfrom [24, 43, 58], we want to predict 5\nfrom [24, 43, 58, 5], we want to predict 57\nfrom [24, 43, 58, 5, 57], we want to predict 1\nfrom [24, 43, 58, 5, 57, 1], we want to predict 46\nfrom [24, 43, 58, 5, 57, 1, 46], we want to predict 43\nfrom [24, 43, 58, 5, 57, 1, 46, 43], we want to predict 39\nfrom [44], we want to predict 53\nfrom [44, 53], we want to predict 56\nfrom [44, 53, 56], we want to predict 1\nfrom [44, 53, 56, 1], we want to predict 58\nfrom [44, 53, 56, 1, 58], we want to predict 46\nfrom [44, 53, 56, 1, 58, 46], we want to predict 39\nfrom [44, 53, 56, 1, 58, 46, 39], we want to predict 58\nfrom [44, 53, 56, 1, 58, 46, 39, 58], we want to predict 1\nfrom [52], we want to predict 58\nfrom [52, 58], we want to predict 1\nfrom [52, 58, 1], we want to predict 58\nfrom [52, 58, 1, 58], we want to predict 46\nfrom [52, 58, 1, 58, 46], we want to predict 39\nfrom [52, 58, 1, 58, 46, 39], we want to predict 58\nfrom [52, 58, 1, 58, 46, 39, 58], we want to predict 1\nfrom [52, 58, 1, 58, 46, 39, 58, 1], we want to predict 46\nfrom [25], we want to predict 17\nfrom [25, 17], we want to predict 27\nfrom [25, 17, 27], we want to predict 10\nfrom [25, 17, 27, 10], we want to predict 0\nfrom [25, 17, 27, 10, 0], we want to predict 21\nfrom [25, 17, 27, 10, 0, 21], we want to predict 1\nfrom [25, 17, 27, 10, 0, 21, 1], we want to predict 54\nfrom [25, 17, 27, 10, 0, 21, 1, 54], we want to predict 39\n",
          "output_type": "stream"
        }
      ],
      "id": "4PHdzkPdwFHZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BiGram Language Model"
      ],
      "metadata": {
        "id": "1sF6TKbuyEUx"
      },
      "id": "1sF6TKbuyEUx"
    },
    {
      "cell_type": "code",
      "source": [
        "emb = torch.nn.Embedding(vocab_size, vocab_size)\n",
        "emb.weight.shape, emb.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOmItthF0ep5",
        "outputId": "dda1d154-a7a4-4bc7-dcc5-bf08845fdf19",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:50.078363Z",
          "iopub.execute_input": "2024-09-09T06:42:50.078935Z",
          "iopub.status.idle": "2024-09-09T06:42:50.122263Z",
          "shell.execute_reply.started": "2024-09-09T06:42:50.078902Z",
          "shell.execute_reply": "2024-09-09T06:42:50.121312Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([65, 65]),\n Parameter containing:\n tensor([[ 0.6258,  0.0255,  0.9545,  ...,  0.0688,  1.3327, -0.4970],\n         [ 0.4658, -0.2573, -1.0673,  ...,  1.2439,  1.3471,  1.6910],\n         [-0.1244, -1.6824,  1.1346,  ...,  1.0785, -0.6150, -0.4589],\n         ...,\n         [ 0.1373,  0.2902, -0.1721,  ...,  2.9050,  1.3809,  0.5141],\n         [ 0.0195,  0.3881,  0.5838,  ...,  0.1362, -0.2022, -1.8831],\n         [-0.5178, -0.0930,  0.7448,  ...,  0.0883,  2.3935, -0.7376]],\n        requires_grad=True))"
          },
          "metadata": {}
        }
      ],
      "id": "xOmItthF0ep5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The torch's [corss_entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) function expects the shape of the inputs to be:\n",
        "* input: (N, C)\n",
        "* targets: (N)\n",
        "\n",
        "So we need to re-shape our logits and targets before passing to cross_entropy()"
      ],
      "metadata": {
        "id": "k5kJ1wJdU8il"
      },
      "id": "k5kJ1wJdU8il"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  \"\"\"The Bigram model only looks at 1 character to predict the next.\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # lookup table of logits of character and its following character\n",
        "    # nn.Embedding manpage: This module is often used to store word embeddings\n",
        "    # and retrieve them using indices. The input to the module is a list of\n",
        "    # indices, and the output is the corresponding word embeddings.\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    \"\"\"idx and targets are both (B, T) tensor of integers\n",
        "    returns logits of (B,T,C) (C is channel, here it's vocab_size)\n",
        "    \"\"\"\n",
        "    logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      # reshape to fit into the torch's cross_entropy function\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T) # or -1\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"idx is (B,T) array of vocab indices in current context\n",
        "    max_new_tokens - number of tokens to generate\n",
        "    return a list of indices of the predicted tokens (from vocab)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, _ = self(idx) # ignore the loss since we are in inference\n",
        "      # we are interested in the last token in the time-series, so only work on that\n",
        "      probs = F.softmax(logits[:, -1, :], dim=-1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append the sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "frkZRCWYyAjR",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:50.123495Z",
          "iopub.execute_input": "2024-09-09T06:42:50.123793Z",
          "iopub.status.idle": "2024-09-09T06:42:50.134468Z",
          "shell.execute_reply.started": "2024-09-09T06:42:50.123762Z",
          "shell.execute_reply": "2024-09-09T06:42:50.133637Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "frkZRCWYyAjR"
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "m.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "logits.shape, loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU3R0uOKRsjA",
        "outputId": "eaf94594-53e6-4282-e0da-7399171019d3",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:50.135595Z",
          "iopub.execute_input": "2024-09-09T06:42:50.135859Z",
          "iopub.status.idle": "2024-09-09T06:42:50.554135Z",
          "shell.execute_reply.started": "2024-09-09T06:42:50.135830Z",
          "shell.execute_reply": "2024-09-09T06:42:50.553200Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([32, 65]),\n tensor(4.8786, device='cuda:0', grad_fn=<NllLossBackward0>))"
          },
          "metadata": {}
        }
      ],
      "id": "GU3R0uOKRsjA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "looking at the initial result: loss=4.8786\n",
        "\n",
        "since we have vocab_size 65, without any training, we'd expect each token has equal chance of being the target, so the loss should be -ln(1/65)=4.17\n",
        "\n",
        "so our loss of 4.8786 is too high, suggesting initialisation is not quite right."
      ],
      "metadata": {
        "id": "P2UDj9NKWIqM"
      },
      "id": "P2UDj9NKWIqM"
    },
    {
      "cell_type": "code",
      "source": [
        "# predict\n",
        "#input = torch.tensor([[20, 43, 50, 50]], dtype=torch.long) # 'Hell', here batch size is 1\n",
        "input = torch.zeros((1,1), dtype=torch.long, device=device) # 0 is \\n\n",
        "decode(m.generate(input, max_new_tokens=100)[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4QcelIBwZk8v",
        "outputId": "2b149185-8d0b-4c85-e194-6907991d02ac",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:50.555330Z",
          "iopub.execute_input": "2024-09-09T06:42:50.555687Z",
          "iopub.status.idle": "2024-09-09T06:42:50.667601Z",
          "shell.execute_reply.started": "2024-09-09T06:42:50.555652Z",
          "shell.execute_reply": "2024-09-09T06:42:50.666686Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 28,
          "output_type": "execute_result",
          "data": {
            "text/plain": "\"\\npYCXxfRkRZd\\nwc'wfNfT;OLlTEeC K\\njxqPToTb?bXAUG:C-SGJO-33SM:C?YI3a\\nhs:LVXJFhXeNuwqhObxZ.tSVrddXlaSZaNe\""
          },
          "metadata": {}
        }
      ],
      "id": "4QcelIBwZk8v"
    },
    {
      "cell_type": "code",
      "source": [
        "# ADAM allows higher lr, other optimizers may need 1e-4 or lower\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "SdN27uMGieqa",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:50.674582Z",
          "iopub.execute_input": "2024-09-09T06:42:50.674896Z",
          "iopub.status.idle": "2024-09-09T06:42:51.970248Z",
          "shell.execute_reply.started": "2024-09-09T06:42:50.674863Z",
          "shell.execute_reply": "2024-09-09T06:42:51.969461Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "SdN27uMGieqa"
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "batch_size=32\n",
        "for steps in range(10000):\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqR7VeBAiwJN",
        "outputId": "22e9438f-ad0c-4e57-d33b-f8822f70e788",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:42:51.971601Z",
          "iopub.execute_input": "2024-09-09T06:42:51.972113Z",
          "iopub.status.idle": "2024-09-09T06:43:11.423458Z",
          "shell.execute_reply.started": "2024-09-09T06:42:51.972063Z",
          "shell.execute_reply": "2024-09-09T06:43:11.422484Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "2.394822835922241\n",
          "output_type": "stream"
        }
      ],
      "id": "cqR7VeBAiwJN"
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXLLSAt3jnZ-",
        "outputId": "3a2488f2-d092-4faf-ace0-0298d5b54a5a",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.424717Z",
          "iopub.execute_input": "2024-09-09T06:43:11.425032Z",
          "iopub.status.idle": "2024-09-09T06:43:11.573471Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.424998Z",
          "shell.execute_reply": "2024-09-09T06:43:11.572575Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nWawice my.\n\nHDERarom oroup\nYowh$Frtof isth ble mil ndill, ath iree sengmin lat Heriliovets, and Win nghir.\nTranousel lind me l.\nHAule ce hiry:\nSupr aisspllw y.\nHerindu n Boopetelaves\nMP:\n\nPl, d mothakleo Windo whth eisbyo the m dourive we higend t so mower; te\n\nAN ad nterupt f s ar igr t m:\n\nThiny aleronth,\nMadPre?\n\nWISo myr f-NLIERor,\nSb&y, wardsal thes ghesthidin cour ay aney Iry ts I&fr y ce.\nJMOn pand, bemary.\nYor 'Wour menm sora anghy t-e nomes twe ten.\nWand thot sulin s th llety ome.\nI muc\n",
          "output_type": "stream"
        }
      ],
      "id": "CXLLSAt3jnZ-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "vu0372H2oEkb"
      },
      "id": "vu0372H2oEkb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "0y6cLjE6oKph"
      },
      "id": "0y6cLjE6oKph"
    },
    {
      "cell_type": "code",
      "source": [
        "# consider this toy example:\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIslZMCRoHIu",
        "outputId": "68ec5c76-ba27-4ef7-9593-7e5f9616268e",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.574474Z",
          "iopub.execute_input": "2024-09-09T06:43:11.574753Z",
          "iopub.status.idle": "2024-09-09T06:43:11.584833Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.574722Z",
          "shell.execute_reply": "2024-09-09T06:43:11.583979Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 32,
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([4, 8, 2])"
          },
          "metadata": {}
        }
      ],
      "id": "pIslZMCRoHIu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "currently each token in the time-series are isolated, not talking to each other we want them to communicate to each other: each token should communicate to prior tokens (but not future ones).\n",
        "\n",
        "What's the simplest way to communicate?\n",
        "* bag of words (BOW): do a sum or average of the current and previous steps, i.e. moving average. But it is a lossy way, e.g. not considering spatial arrangements of those tokens"
      ],
      "metadata": {
        "id": "ZAV7D5BlpLik"
      },
      "id": "ZAV7D5BlpLik"
    },
    {
      "cell_type": "code",
      "source": [
        "# version 1 of self-attention\n",
        "# we want x[b,t] = mean of x[b, i] where i is 0 to t\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t+1]  # (t, C)\n",
        "    xbow[b, t] = xprev.mean(dim=0)"
      ],
      "metadata": {
        "id": "SxcVztkIowbn",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.586272Z",
          "iopub.execute_input": "2024-09-09T06:43:11.586858Z",
          "iopub.status.idle": "2024-09-09T06:43:11.595350Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.586814Z",
          "shell.execute_reply": "2024-09-09T06:43:11.594392Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "SxcVztkIowbn"
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, xbow.shape, x[0], xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50EqG-O2q_k2",
        "outputId": "6d0e7c70-ead5-4f24-c0c9-bcf76054de6b",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.596576Z",
          "iopub.execute_input": "2024-09-09T06:43:11.597015Z",
          "iopub.status.idle": "2024-09-09T06:43:11.606725Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.596972Z",
          "shell.execute_reply": "2024-09-09T06:43:11.605840Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 34,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([4, 8, 2]),\n torch.Size([4, 8, 2]),\n tensor([[ 0.1808, -0.0700],\n         [-0.3596, -0.9152],\n         [ 0.6258,  0.0255],\n         [ 0.9545,  0.0643],\n         [ 0.3612,  1.1679],\n         [-1.3499, -0.5102],\n         [ 0.2360, -0.2398],\n         [-0.9211,  1.5433]]),\n tensor([[ 0.1808, -0.0700],\n         [-0.0894, -0.4926],\n         [ 0.1490, -0.3199],\n         [ 0.3504, -0.2238],\n         [ 0.3525,  0.0545],\n         [ 0.0688, -0.0396],\n         [ 0.0927, -0.0682],\n         [-0.0341,  0.1332]]))"
          },
          "metadata": {}
        }
      ],
      "id": "50EqG-O2q_k2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above way is very inefficient. Hence the trick."
      ],
      "metadata": {
        "id": "kHf8YMl365ZO"
      },
      "id": "kHf8YMl365ZO"
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the toy example: using tril gives us the cumulative sum\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c= a@b\n",
        "a, b, c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VT8GE-y6_sv",
        "outputId": "528bf0cc-f9ed-4fae-d15a-abe353e7abeb",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.607840Z",
          "iopub.execute_input": "2024-09-09T06:43:11.608104Z",
          "iopub.status.idle": "2024-09-09T06:43:11.657072Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.608076Z",
          "shell.execute_reply": "2024-09-09T06:43:11.656230Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 35,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(tensor([[1., 0., 0.],\n         [1., 1., 0.],\n         [1., 1., 1.]]),\n tensor([[2., 7.],\n         [6., 4.],\n         [6., 5.]]),\n tensor([[ 2.,  7.],\n         [ 8., 11.],\n         [14., 16.]]))"
          },
          "metadata": {}
        }
      ],
      "id": "5VT8GE-y6_sv"
    },
    {
      "cell_type": "code",
      "source": [
        "# now we avarage a and try the same\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, dim=1, keepdims=True)\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c= a@b\n",
        "a, b, c\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAmxdRGJ74gc",
        "outputId": "e90368b9-3f0c-4ca7-ef62-cd115f6ca2d2",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.658177Z",
          "iopub.execute_input": "2024-09-09T06:43:11.658790Z",
          "iopub.status.idle": "2024-09-09T06:43:11.667500Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.658757Z",
          "shell.execute_reply": "2024-09-09T06:43:11.666451Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(tensor([[1.0000, 0.0000, 0.0000],\n         [0.5000, 0.5000, 0.0000],\n         [0.3333, 0.3333, 0.3333]]),\n tensor([[0., 4.],\n         [0., 3.],\n         [8., 4.]]),\n tensor([[0.0000, 4.0000],\n         [0.0000, 3.5000],\n         [2.6667, 3.6667]]))"
          },
          "metadata": {}
        }
      ],
      "id": "PAmxdRGJ74gc"
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2\n",
        "# equivalent of the above for loops using the trick:\n",
        "# Create a lower triangular matrix of 1s\n",
        "mask = torch.tril(torch.ones(T, T))  # Shape: (T, T)\n",
        "mask = mask / mask.sum(1, keepdim=True)\n",
        "\n",
        "xbow2 = mask @ x"
      ],
      "metadata": {
        "id": "ALIhR6ams_D7",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.668745Z",
          "iopub.execute_input": "2024-09-09T06:43:11.669049Z",
          "iopub.status.idle": "2024-09-09T06:43:11.676026Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.669016Z",
          "shell.execute_reply": "2024-09-09T06:43:11.675183Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "ALIhR6ams_D7"
    },
    {
      "cell_type": "code",
      "source": [
        "xbow2.shape, xbow[1], xbow2[1], torch.allclose(xbow, xbow2), xbow-xbow2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWH6I8stxYE7",
        "outputId": "10534dfe-814c-4f51-b9a3-61a6f7f2bdc3",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.677182Z",
          "iopub.execute_input": "2024-09-09T06:43:11.677668Z",
          "iopub.status.idle": "2024-09-09T06:43:11.690396Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.677632Z",
          "shell.execute_reply": "2024-09-09T06:43:11.689473Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 38,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([4, 8, 2]),\n tensor([[ 1.3488, -0.1396],\n         [ 0.8173,  0.4127],\n         [-0.1342,  0.4395],\n         [ 0.2711,  0.4774],\n         [ 0.2421,  0.0694],\n         [ 0.0084,  0.0020],\n         [ 0.0712, -0.1128],\n         [ 0.2527,  0.2149]]),\n tensor([[ 1.3488, -0.1396],\n         [ 0.8173,  0.4127],\n         [-0.1342,  0.4395],\n         [ 0.2711,  0.4774],\n         [ 0.2421,  0.0694],\n         [ 0.0084,  0.0020],\n         [ 0.0712, -0.1128],\n         [ 0.2527,  0.2149]]),\n False,\n tensor([[[ 0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  2.9802e-08],\n          [ 0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  7.4506e-09],\n          [-7.4506e-09,  0.0000e+00],\n          [ 7.4506e-09, -1.4901e-08],\n          [ 0.0000e+00,  0.0000e+00]],\n \n         [[ 0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00],\n          [-1.4901e-08,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00],\n          [-1.4901e-08,  2.2352e-08],\n          [-4.6566e-09,  3.2363e-08],\n          [-7.4506e-09,  0.0000e+00],\n          [ 2.9802e-08,  0.0000e+00]],\n \n         [[ 0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00],\n          [-1.4901e-08, -2.9802e-08],\n          [ 0.0000e+00,  0.0000e+00],\n          [ 1.4901e-08, -7.4506e-09],\n          [ 7.4506e-09,  0.0000e+00],\n          [ 2.9802e-08,  2.9802e-08],\n          [-2.9802e-08,  2.9802e-08]],\n \n         [[ 0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  0.0000e+00],\n          [ 0.0000e+00,  2.9802e-08],\n          [ 0.0000e+00, -1.4901e-08],\n          [ 0.0000e+00,  2.9802e-08]]]))"
          },
          "metadata": {}
        }
      ],
      "id": "jWH6I8stxYE7"
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use softmax\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf')) # cannot communicate with future\n",
        "wei = F.softmax(wei, dim=1) # softmax takes ln(), then average\n",
        "xbow3 = wei@x\n",
        "\n",
        "wei, xbow3-xbow2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy_XEFLd_PS8",
        "outputId": "82a59507-e8b7-4698-b7af-0ca34dd73a1b",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.691560Z",
          "iopub.execute_input": "2024-09-09T06:43:11.691982Z",
          "iopub.status.idle": "2024-09-09T06:43:11.706542Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.691941Z",
          "shell.execute_reply": "2024-09-09T06:43:11.705795Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 39,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n         [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n         [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]),\n tensor([[[0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.]],\n \n         [[0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.]],\n \n         [[0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.]],\n \n         [[0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.]]]))"
          },
          "metadata": {}
        }
      ],
      "id": "uy_XEFLd_PS8"
    },
    {
      "cell_type": "code",
      "source": [
        "tril"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx69Q2ce_xNu",
        "outputId": "67e4b235-2cd4-452f-d806-97bfc1d56726",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.707549Z",
          "iopub.execute_input": "2024-09-09T06:43:11.707813Z",
          "iopub.status.idle": "2024-09-09T06:43:11.714649Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.707783Z",
          "shell.execute_reply": "2024-09-09T06:43:11.713670Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 40,
          "output_type": "execute_result",
          "data": {
            "text/plain": "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1.]])"
          },
          "metadata": {}
        }
      ],
      "id": "Wx69Q2ce_xNu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Attention\n",
        "\n",
        "x (embedding+position) is private information about the token. With a self-attention head, every single token will emit 3 vectors:\n",
        "* Query - what am I looking for, e.g. being a vowel, I am looking for any consonent up to position 4...\n",
        "* Key - what do I contain.\n",
        "* Value - if you find me interesting, here is what I communicate to you - what's aggregated for the purpose of the single head between the different nodes/tokens.\n",
        "\n",
        "The way we do affinities between tokens is basically do dot product between keys and queries - i.e. my Q dot prod with all the K's of other tokens. That dot prod becomes wei in our example. If the Q and K match they create a high affinity (i.e. similar values in wei).\n",
        "![Screenshot 2024-09-07 132749.jpg](data:image/jpeg;base64,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)\n",
        "\n",
        "Notes:\n",
        "\n",
        "* Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "* There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "* Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "* In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "* \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "* \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "NWGrxtFjbWDv"
      },
      "id": "NWGrxtFjbWDv"
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4 - self-attention\n",
        "torch.manual_seed(1337)\n",
        "B,T,C=4, 8, 32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# implement a single head of the self-attention\n",
        "head_size=16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B,T,16)\n",
        "q = query(x) # (B,T,16)\n",
        "v = value(x) # (B,T,16)\n",
        "# wei is the affinity between k and q:\n",
        "wei = q @ k.transpose(-2, -1) # (B,T,16)@(B,16,T)->(B,T,T)\n",
        "#wei = torch.zeros((T,T))\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "# we don't need the following line in the encoder block. It's needed in decoder block.\n",
        "wei = wei.masked_fill(tril==0, float('-inf')) # cannot communicate with future\n",
        "wei = F.softmax(wei, dim=-1) # softmax takes ln(), then average\n",
        "\n",
        "# out = wei@x # (B,T,C)\n",
        "out = wei@v # (B,T,T)@(B,T,16)->(B,T,16)"
      ],
      "metadata": {
        "id": "Xhzdd37JJI71",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.715552Z",
          "iopub.execute_input": "2024-09-09T06:43:11.715839Z",
          "iopub.status.idle": "2024-09-09T06:43:11.730394Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.715808Z",
          "shell.execute_reply": "2024-09-09T06:43:11.729456Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "Xhzdd37JJI71"
    },
    {
      "cell_type": "code",
      "source": [
        "wei.shape, out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKMd_5ekMNM9",
        "outputId": "3f13384d-fcd8-4689-9773-371e5d72b3fe",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.731653Z",
          "iopub.execute_input": "2024-09-09T06:43:11.731928Z",
          "iopub.status.idle": "2024-09-09T06:43:11.738341Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.731898Z",
          "shell.execute_reply": "2024-09-09T06:43:11.737403Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 42,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([4, 8, 8]), torch.Size([4, 8, 16]))"
          },
          "metadata": {}
        }
      ],
      "id": "eKMd_5ekMNM9"
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0], out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxjZuT4DXhx3",
        "outputId": "ab58330d-db6d-4890-924e-cb964dc82c89",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.739441Z",
          "iopub.execute_input": "2024-09-09T06:43:11.739744Z",
          "iopub.status.idle": "2024-09-09T06:43:11.751136Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.739709Z",
          "shell.execute_reply": "2024-09-09T06:43:11.750262Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 43,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n        grad_fn=<SelectBackward0>),\n tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n          -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n         [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n          -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n         [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n          -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n         [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n          -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n         [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n          -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n         [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n          -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n         [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n          -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n         [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n           0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n        grad_fn=<SelectBackward0>))"
          },
          "metadata": {}
        }
      ],
      "id": "BxjZuT4DXhx3"
    },
    {
      "cell_type": "code",
      "source": [
        "# why divide by sqrt(head_size)?\n",
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size ** -0.5\n",
        "k.var(), q.var(), wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BzWFd4TkW4A",
        "outputId": "84053a53-c623-4994-e570-b389e2c0fce5",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.752334Z",
          "iopub.execute_input": "2024-09-09T06:43:11.752639Z",
          "iopub.status.idle": "2024-09-09T06:43:11.762614Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.752604Z",
          "shell.execute_reply": "2024-09-09T06:43:11.761713Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 44,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(tensor(1.0449), tensor(1.0700), tensor(1.0918))"
          },
          "metadata": {}
        }
      ],
      "id": "4BzWFd4TkW4A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without the scaline, wei.var() is 17.4... around the value of head_size. When doing softmax, we want the input values to be diffused, otherwise, the softmax will spike on the peak values and become one-hot. So we scale wei to make it unit Gaussian."
      ],
      "metadata": {
        "id": "tByBzQbUk-nI"
      },
      "id": "tByBzQbUk-nI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip / Resudual connection\n",
        "\n",
        "Originated from paper [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)\n",
        "\n",
        "In backprop, addition distributes the gradient equally to each path/branch. So the skip connection is a information superhighway for gradient to travel from supervision (loss) back to the input unimpeded. It dramatically helps with optimisation."
      ],
      "metadata": {
        "id": "1iwNV1CP5Gks"
      },
      "id": "1iwNV1CP5Gks"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it together\n",
        "<img src='https://miro.medium.com/v2/resize:fit:720/format:webp/1*BHzGVskWGS_3jEcYYi6miQ.png' width='400px'>\n",
        "\n",
        "Out implementation only has decoder, no encoder or the cross-attention block."
      ],
      "metadata": {
        "id": "j9WtuKpbBCQj"
      },
      "id": "j9WtuKpbBCQj"
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "block_size=8\n",
        "max_iters=5000\n",
        "eval_interval=500\n",
        "lr = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "emb_dim = 32 # embedding dimensions\n",
        "head_size=None"
      ],
      "metadata": {
        "id": "-cjyFTYBBYXG",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.763985Z",
          "iopub.execute_input": "2024-09-09T06:43:11.764280Z",
          "iopub.status.idle": "2024-09-09T06:43:11.769848Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.764248Z",
          "shell.execute_reply": "2024-09-09T06:43:11.768691Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "-cjyFTYBBYXG"
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out={}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "10nig0hUDkt_",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.771058Z",
          "iopub.execute_input": "2024-09-09T06:43:11.771987Z",
          "iopub.status.idle": "2024-09-09T06:43:11.781395Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.771945Z",
          "shell.execute_reply": "2024-09-09T06:43:11.780332Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "10nig0hUDkt_"
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\"Single head self-attention\n",
        "  \"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(emb_dim, head_size, bias=False)\n",
        "    self.query = nn.Linear(emb_dim, head_size, bias=False)\n",
        "    self.value = nn.Linear(emb_dim, head_size, bias=False)\n",
        "    # since tril is not a native torch Module member, we use register_buffer\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"x is (B,T,C)\n",
        "    \"\"\"\n",
        "    B,T,C=x.shape\n",
        "    k = self.key(x) # (B,T,H) where H is head_size\n",
        "    q = self.key(x) # (B,T,H)\n",
        "    v = self.key(x) # (B,T,H)\n",
        "    _,_,H = k.shape\n",
        "    wei = q @ k.transpose(-2, -1) * H ** -0.5 # (B,T,H)@(B,H,T)->(B,T,T)\n",
        "    # the [:T,:T] is necessary as x can be (1,1,C) during inference\n",
        "    wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B,T,T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
        "    out = wei @ v # (B,T,T)@(B,T,H)->(B,T,H)\n",
        "    return out"
      ],
      "metadata": {
        "id": "wsgRyE6bmVcy",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.782702Z",
          "iopub.execute_input": "2024-09-09T06:43:11.783074Z",
          "iopub.status.idle": "2024-09-09T06:43:11.793576Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.783037Z",
          "shell.execute_reply": "2024-09-09T06:43:11.792633Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "wsgRyE6bmVcy"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList( [Head(head_size) for _ in range(num_heads)] )\n",
        "    # add another projection layer for going back to the residual pathway\n",
        "    self.proj = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat( [h(x) for h in self.heads], dim=-1 )\n",
        "    return self.proj(out)"
      ],
      "metadata": {
        "id": "Y-JxjAzvy_je",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.795015Z",
          "iopub.execute_input": "2024-09-09T06:43:11.795444Z",
          "iopub.status.idle": "2024-09-09T06:43:11.808253Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.795382Z",
          "shell.execute_reply": "2024-09-09T06:43:11.807464Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "Y-JxjAzvy_je"
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\"After self-attention (which communicates among tokens), the feed forward\n",
        "  layer in Transformer allows the tokens/nodes to think indivicually before\n",
        "  output layer.\n",
        "  \"\"\"\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        # in Transformer whitepaper, the FFN inner layer dim is 2048, the input\n",
        "        # dim is 512, i.e. 4 times the input dim. So we do the same here.\n",
        "        nn.Linear(n_embed, 4 * n_embed),\n",
        "        nn.ReLU(),\n",
        "        # add another projection layer for going back to the residual pathway\n",
        "        nn.Linear(4 * n_embed, n_embed),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "Avm0uIDH2Y1V",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.809787Z",
          "iopub.execute_input": "2024-09-09T06:43:11.810127Z",
          "iopub.status.idle": "2024-09-09T06:43:11.819695Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.810087Z",
          "shell.execute_reply": "2024-09-09T06:43:11.818759Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "Avm0uIDH2Y1V"
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_heads):\n",
        "    super().__init__()\n",
        "    self.sa_heads = MultiHeadAttention(n_heads, n_embed//n_heads)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa_heads( self.ln1(x) )\n",
        "    x = x + self.ffwd( self.ln2(x) )\n",
        "    return x"
      ],
      "metadata": {
        "id": "BP1vc6hu6PTx",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.820918Z",
          "iopub.execute_input": "2024-09-09T06:43:11.821194Z",
          "iopub.status.idle": "2024-09-09T06:43:11.833860Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.821163Z",
          "shell.execute_reply": "2024-09-09T06:43:11.833057Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "BP1vc6hu6PTx"
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm1d:\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    \"\"\"with Wevenet, the dim can be 2-dimensional.\n",
        "    \"\"\"\n",
        "    self.eps=eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    \"\"\"with Wavenet, x can be 3-dimensional (B,V,C), instead of 2 (B,V*C).\n",
        "    for 3-dim x, we want to do the mean and var along the 1st 2 dimensions,\n",
        "    i.e. x.view(-1, last_dim).mean(dim=0, keepdim=True)\n",
        "    torch's mean can specify the dim as tuple, so we can use (0,1) as the dim\n",
        "    when calling mean, and achieve the same.\n",
        "    \"\"\"\n",
        "    if self.training:\n",
        "      if x.ndim==2:\n",
        "        dim=0\n",
        "      if x.ndim==3:\n",
        "        dim=(0,1)\n",
        "      xmean = x.mean(dim, keepdim=True)\n",
        "      xvar = x.var(dim, keepdim=True)\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1-self.momentum)*self.running_mean + self.momentum*xmean\n",
        "        self.running_var = (1-self.momentum)*self.running_var + self.momentum*xvar\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "\n",
        "    x_hat = (x - xmean) / (xvar + self.eps)**0.5\n",
        "    self.out = self.gamma * x_hat + self.beta\n",
        "\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "ekxjFrpzBRzl",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.835306Z",
          "iopub.execute_input": "2024-09-09T06:43:11.835711Z",
          "iopub.status.idle": "2024-09-09T06:43:11.846504Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.835668Z",
          "shell.execute_reply": "2024-09-09T06:43:11.845563Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "ekxjFrpzBRzl"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "module = BatchNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch 32, of 100-dim vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5NxbsxxBWZ6",
        "outputId": "dc5c1d16-40ae-4782-9b73-439cdecac56e",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.847670Z",
          "iopub.execute_input": "2024-09-09T06:43:11.847961Z",
          "iopub.status.idle": "2024-09-09T06:43:11.872334Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.847929Z",
          "shell.execute_reply": "2024-09-09T06:43:11.871522Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 52,
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([32, 100])"
          },
          "metadata": {}
        }
      ],
      "id": "X5NxbsxxBWZ6"
    },
    {
      "cell_type": "code",
      "source": [
        "# batch norm makes each node unit-Gaussian across the batch (dim 0).\n",
        "x.var(dim=0), x.mean(dim=0), x.std(dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vea7sAnWBzRr",
        "outputId": "f3597e2b-c2c4-4d08-dd07-4fc7f0ee1234",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.873401Z",
          "iopub.execute_input": "2024-09-09T06:43:11.873736Z",
          "iopub.status.idle": "2024-09-09T06:43:11.884701Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.873700Z",
          "shell.execute_reply": "2024-09-09T06:43:11.883803Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 53,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000]),\n tensor([-7.4506e-09,  7.4506e-08,  1.1176e-08,  2.7940e-09,  3.7253e-09,\n          2.2352e-08,  0.0000e+00,  2.2352e-08,  2.9802e-08, -4.4703e-08,\n          0.0000e+00, -3.3528e-08,  7.4506e-09, -3.7253e-09,  2.2352e-08,\n         -3.7253e-09,  7.4506e-09,  0.0000e+00,  0.0000e+00, -2.6077e-08,\n          2.2352e-08, -1.7695e-08, -7.4506e-09,  2.2352e-08, -2.9802e-08,\n         -2.2352e-08, -2.2352e-08, -1.4901e-08,  7.4506e-09, -7.4506e-09,\n          1.7695e-08,  6.5193e-09,  1.8626e-09, -5.5879e-09, -1.4901e-08,\n          1.4901e-08,  0.0000e+00, -1.8626e-09,  2.2352e-08, -1.4901e-08,\n          1.4901e-08,  0.0000e+00, -1.4901e-08,  0.0000e+00,  2.6077e-08,\n          0.0000e+00,  0.0000e+00,  0.0000e+00, -2.2352e-08, -2.9802e-08,\n          0.0000e+00,  3.7253e-09,  2.2352e-08,  7.4506e-09,  1.4901e-08,\n          0.0000e+00,  2.9802e-08, -2.2352e-08, -2.7940e-08, -1.1176e-08,\n          3.7253e-08,  1.4901e-08,  1.4901e-08, -3.3528e-08,  1.3039e-08,\n          1.4901e-08, -2.2352e-08, -2.9802e-08,  1.8626e-09, -1.3039e-08,\n         -7.4506e-09, -1.4901e-08, -1.4901e-08, -1.8626e-08, -2.9802e-08,\n         -2.9802e-08, -7.4506e-09, -1.4901e-08,  7.4506e-09, -1.1176e-08,\n          7.4506e-09,  1.4901e-08, -3.7253e-08, -1.7695e-08, -8.9407e-08,\n         -1.4901e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.6566e-09,\n          1.4901e-08,  2.3283e-09,  1.4901e-08, -5.2154e-08,  3.7253e-08,\n          0.0000e+00, -5.5879e-09, -1.4901e-08,  1.4901e-08,  1.4901e-08]),\n tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000]))"
          },
          "metadata": {}
        }
      ],
      "id": "Vea7sAnWBzRr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implementation of LayerNorm is same as BatchNorm, except for when calculating the mean() and variance, we use dim 1 instead of 0.\n",
        "\n",
        "Also, we don't need to maintain the batch-wide running means and running var during training.\n",
        "\n",
        "Note, unlike in the original whitepaper, nowadays, we apply the LayerNorm before the transformation. This is called per-norm formulation."
      ],
      "metadata": {
        "id": "FaQxOx0xCz4f"
      },
      "id": "FaQxOx0xCz4f"
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm:\n",
        "  def __init__(self, dim, eps=1e-5):\n",
        "    \"\"\"with Wevenet, the dim can be 2-dimensional.\n",
        "    \"\"\"\n",
        "    self.eps=eps\n",
        "\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    \"\"\"with Wavenet, x can be 3-dimensional (B,V,C), instead of 2 (B,V*C).\n",
        "    for 3-dim x, we want to do the mean and var along the 1st 2 dimensions,\n",
        "    i.e. x.view(-1, last_dim).mean(dim=0, keepdim=True)\n",
        "    torch's mean can specify the dim as tuple, so we can use (0,1) as the dim\n",
        "    when calling mean, and achieve the same.\n",
        "    \"\"\"\n",
        "    if x.ndim==2:\n",
        "      dim=1       # chagne to dim=1 for layer norm\n",
        "    if x.ndim==3:\n",
        "      dim=(1,1)   # chagne to dim=1 for layer norm\n",
        "    xmean = x.mean(dim, keepdim=True)\n",
        "    xvar = x.var(dim, keepdim=True)\n",
        "\n",
        "    x_hat = (x - xmean) / (xvar + self.eps)**0.5\n",
        "    self.out = self.gamma * x_hat + self.beta\n",
        "\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "MtQFMnfbCv2c",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.886034Z",
          "iopub.execute_input": "2024-09-09T06:43:11.886559Z",
          "iopub.status.idle": "2024-09-09T06:43:11.896695Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.886524Z",
          "shell.execute_reply": "2024-09-09T06:43:11.895835Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "MtQFMnfbCv2c"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "module = LayerNorm(100)\n",
        "x = torch.randn(32, 100) # batch 32, of 100-dim vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfgQ4lkCDKtc",
        "outputId": "7cdc5aa7-eb7b-4a5e-8404-8f87713032ff",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.899583Z",
          "iopub.execute_input": "2024-09-09T06:43:11.899861Z",
          "iopub.status.idle": "2024-09-09T06:43:11.911237Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.899831Z",
          "shell.execute_reply": "2024-09-09T06:43:11.910334Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 55,
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([32, 100])"
          },
          "metadata": {}
        }
      ],
      "id": "sfgQ4lkCDKtc"
    },
    {
      "cell_type": "code",
      "source": [
        "# batch norm makes each node unit-Gaussian across the batch (dim 1).\n",
        "x.var(dim=1), x.mean(dim=1), x.std(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTNukwl4DPaL",
        "outputId": "acc3ed62-6163-42e7-de4e-c1cd4a80a4b9",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.912980Z",
          "iopub.execute_input": "2024-09-09T06:43:11.913328Z",
          "iopub.status.idle": "2024-09-09T06:43:11.923492Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.913287Z",
          "shell.execute_reply": "2024-09-09T06:43:11.922363Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 56,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n tensor([-9.5367e-09, -2.3842e-09, -2.0266e-08,  1.7881e-08,  1.6689e-08,\n          9.8348e-09,  4.7684e-09,  1.9073e-08, -1.4305e-08, -4.7684e-09,\n         -1.3113e-08, -5.9605e-09,  0.0000e+00, -7.1526e-09, -2.0266e-08,\n          7.0035e-09, -1.2815e-08,  1.7881e-08,  6.5565e-09, -4.7684e-09,\n          9.5367e-09, -3.5763e-09, -2.8610e-08,  4.7684e-09,  3.5763e-09,\n         -7.1526e-09, -4.7684e-09,  0.0000e+00,  5.3644e-09, -1.1921e-08,\n          4.7684e-09,  1.9073e-08]),\n tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]))"
          },
          "metadata": {}
        }
      ],
      "id": "cTNukwl4DPaL"
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  \"\"\"The Bigram model only looks at 1 character to predict the next.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # lookup table of logits of character and its following character\n",
        "    # nn.Embedding manpage: This module is often used to store word embeddings\n",
        "    # and retrieve them using indices. The input to the module is a list of\n",
        "    # indices, and the output is the corresponding word embeddings.\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, emb_dim)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, emb_dim)\n",
        "\n",
        "    #self.sa_head = Head(emb_dim) # make the head_size same as emb_dim\n",
        "    # self.sa_heads = MultiHeadAttention(4, emb_dim // 4) # 4 heads of 8-dim'l self-attention\n",
        "    # self.ffwd = FeedForward(emb_dim)\n",
        "    self.blocks = nn.Sequential(\n",
        "         Block(emb_dim, 4),\n",
        "         Block(emb_dim, 4),\n",
        "         Block(emb_dim, 4),\n",
        "         nn.LayerNorm(emb_dim),\n",
        "    )\n",
        "\n",
        "    self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    \"\"\"idx and targets are both (B, T) tensor of integers\n",
        "    returns logits of (B,T,C) (C is channel, here it's same as emb_dim)\n",
        "    \"\"\"\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # (B,T,C=emb_dim)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb # (B,T,C)\n",
        "\n",
        "    # x = self.sa_head(x) # (B,T,H) here H==C\n",
        "    # x = self.sa_heads(x) # (B,T,C)\n",
        "    # x = self.ffwd(x) # (B,T,C)\n",
        "    x = self.blocks(x)\n",
        "\n",
        "    logits = self.lm_head(x) # (B,T,C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      # reshape to fit into the torch's cross_entropy function\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T) # or -1\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"idx is (B,T) array of vocab indices in current context\n",
        "    max_new_tokens - number of tokens to generate\n",
        "    return a list of indices of the predicted tokens (from vocab)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      # in case idx is too long, we chop it to the last block_size of tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, _ = self(idx_cond) # ignore the loss since we are in inference\n",
        "      # we are interested in the last token in the time-series, so only work on that\n",
        "      probs = F.softmax(logits[:, -1, :], dim=-1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append the sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "RuBrmiv8BFow",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.925177Z",
          "iopub.execute_input": "2024-09-09T06:43:11.925583Z",
          "iopub.status.idle": "2024-09-09T06:43:11.938133Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.925539Z",
          "shell.execute_reply": "2024-09-09T06:43:11.937275Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "RuBrmiv8BFow"
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "# training\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  # every once in a while evaluate the loss on train and val datasets\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B20dHSCWClwq",
        "outputId": "755f4778-ee47-4024-98db-6164c7b579ab",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:43:11.939376Z",
          "iopub.execute_input": "2024-09-09T06:43:11.939891Z",
          "iopub.status.idle": "2024-09-09T06:44:47.555173Z",
          "shell.execute_reply.started": "2024-09-09T06:43:11.939848Z",
          "shell.execute_reply": "2024-09-09T06:44:47.554203Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "step 0: train loss 4.3675, val loss 4.3653\nstep 500: train loss 2.4852, val loss 2.4732\nstep 1000: train loss 2.3710, val loss 2.3722\nstep 1500: train loss 2.2787, val loss 2.2967\nstep 2000: train loss 2.2442, val loss 2.2609\nstep 2500: train loss 2.1963, val loss 2.2406\nstep 3000: train loss 2.1770, val loss 2.2173\nstep 3500: train loss 2.1605, val loss 2.1799\nstep 4000: train loss 2.1271, val loss 2.1663\nstep 4500: train loss 2.1192, val loss 2.1833\nstep 4999: train loss 2.1192, val loss 2.1833\n",
          "output_type": "stream"
        }
      ],
      "id": "B20dHSCWClwq"
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfOL_F6PvFm6",
        "outputId": "d3f474ac-6fff-47db-fccd-39af3814abd5",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:44:47.556493Z",
          "iopub.execute_input": "2024-09-09T06:44:47.556806Z",
          "iopub.status.idle": "2024-09-09T06:44:57.833160Z",
          "shell.execute_reply.started": "2024-09-09T06:44:47.556773Z",
          "shell.execute_reply": "2024-09-09T06:44:57.832204Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n\nYCIOLIORD:\nTcown,\nThe lay ble\n\nHist my be toe.\nShirs I Muchaveanss:\nWanthir usqurt ther cedelassanes with my.\n\n\nDEENY I IIN Thave some fort heart mal; dill, at miree senecin;\nStist in ovets, and the noper.\n\nWan I elplind thave thuse courvey:\nSup; ais all, yean's naus mamopetelaves\nMometll, demethak\nTo Wind tourt evibys the moldstion and hime sto-of gremeste\nThat dantert,\nIf son; igr therf thre male onth,\nMad sir, I Sometry I sop!\n\nHerblis:\nSadst Wace Eght thisin cour ay and\nHiry thelsen thou his nepens, and griting I on I that\nThat heyser--ponges.\n\nSICING Sand thouse lines the well ome.\n\n\nCORINIUp:\nItst\nDthas lome\nItias detlul wath.\n\nLARIOPEAB:\nHaly to imip soce of's cok heit some so not heare fell tas nyibet' loovers!\n\nRour no duche nee, our as and batake bath whill\n's am s sleve buml nom,\nWhos well nooth ighticks hIneard?\nTheill\nMy thean'en,\nThus?\n\nGroiveth oour iund thouning these man iplytanou what thave thers,\nsterel olbker of soo O\nFord RIIf Bwat dotive wout it fore;\nTo knot ouee, rucan it we heall, I Qull no whe wher the is, do priveftanoun,\nTheracth I his well bemesse mines sofee wortingin it- to hight Whon wow now hit but brate brice nInirst a theint do may, the any inke it blamenattely trupeeing,\nSwill thell.\n\nKING IIIICH:\nthave thou for Heaing tapplul broop!\nAnd folitctest just dee the thre hast, arll bambs,\nWhate il s; doubll tor doth'st they sild the beer seece not bust they gatel veab ele hoble muth the nupt ome whichte your lel!\nHering yie of RI Rarke old the\ner bem sisting souke themer, sares somincts thouk viresceer:\nOn!\n\nThatAnd wheresed well, strare gee saced.\n\nGLORWARD:\nThy.\n\n\nNG SART:\nTill!\nWHeresen rould. ar?\n\nJYG BIIA:\nWhas fet couft his hemoraster no, hat pausht carl brnoten: the wifll,\nBy fort that corr, thou the hamt, lir.\n\nMy thee wuat repeses bef foll sayour!\n\nCUp:\nI'll 'linser liften souos food houst theeak.\n\nSA:\nGath.\n\nGLIO:\nPry promblak now wonsent,\nBut hat wance stan,\nJires woathing:\nInkin shown's for nomme suppecoten'sher be a \n'tir\n",
          "output_type": "stream"
        }
      ],
      "id": "vfOL_F6PvFm6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scale up\n",
        "\n",
        "added [dropouts](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf), changed hyper-param\n",
        "\n",
        "Train this on TPU/GPU."
      ],
      "metadata": {
        "id": "bDJ_rU0XHEB0"
      },
      "id": "bDJ_rU0XHEB0"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size=64   # 16\n",
        "block_size=256  # 32\n",
        "max_iters=5000\n",
        "eval_interval=100\n",
        "lr = 3e-4       # 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "emb_dim = 384   # 64 # embedding dimensions\n",
        "n_heads = 6     # 4\n",
        "n_blocks = 6    # 4\n",
        "dropout = 0.2   # 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split='train'):\n",
        "  \"\"\"split is one of 'train', 'val'\n",
        "  \"\"\"\n",
        "  data = data_train if split=='train' else data_val\n",
        "  batch_ix = torch.randint(0, len(data)-block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in batch_ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in batch_ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out={}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\"Single head self-attention\n",
        "  \"\"\"\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(emb_dim, head_size, bias=False)\n",
        "    self.query = nn.Linear(emb_dim, head_size, bias=False)\n",
        "    self.value = nn.Linear(emb_dim, head_size, bias=False)\n",
        "    # since tril is not a native torch Module member, we use register_buffer\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"x is (B,T,C)\n",
        "    \"\"\"\n",
        "    B,T,C=x.shape\n",
        "    k = self.key(x) # (B,T,H) where H is head_size\n",
        "    q = self.key(x) # (B,T,H)\n",
        "    v = self.key(x) # (B,T,H)\n",
        "    _,_,H = k.shape\n",
        "    wei = q @ k.transpose(-2, -1) * H ** -0.5 # (B,T,H)@(B,H,T)->(B,T,T)\n",
        "    # the [:T,:T] is necessary as x can be (1,1,C) during inference\n",
        "    wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B,T,T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
        "    wei = self.dropout(wei)\n",
        "    out = wei @ v # (B,T,T)@(B,T,H)->(B,T,H)\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList( [Head(head_size) for _ in range(num_heads)] )\n",
        "    # add another projection layer for going back to the residual pathway\n",
        "    self.proj = nn.Linear(emb_dim, emb_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat( [h(x) for h in self.heads], dim=-1 )\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\"After self-attention (which communicates among tokens), the feed forward\n",
        "  layer in Transformer allows the tokens/nodes to think indivicually before\n",
        "  output layer.\n",
        "  \"\"\"\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        # in Transformer whitepaper, the FFN inner layer dim is 2048, the input\n",
        "        # dim is 512, i.e. 4 times the input dim. So we do the same here.\n",
        "        nn.Linear(n_embed, 4 * n_embed),\n",
        "        nn.ReLU(),\n",
        "        # add another projection layer for going back to the residual pathway\n",
        "        nn.Linear(4 * n_embed, n_embed),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_heads):\n",
        "    super().__init__()\n",
        "    self.sa_heads = MultiHeadAttention(n_heads, n_embed//n_heads)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa_heads( self.ln1(x) )\n",
        "    x = x + self.ffwd( self.ln2(x) )\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  \"\"\"The Bigram model only looks at 1 character to predict the next.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # lookup table of logits of character and its following character\n",
        "    # nn.Embedding manpage: This module is often used to store word embeddings\n",
        "    # and retrieve them using indices. The input to the module is a list of\n",
        "    # indices, and the output is the corresponding word embeddings.\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, emb_dim)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, emb_dim)\n",
        "    self.blocks = nn.Sequential(*[Block(emb_dim, n_heads) for _ in range(n_blocks)])\n",
        "    self.ln_f = nn.LayerNorm(emb_dim) # final layerNorm\n",
        "    self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    \"\"\"idx and targets are both (B, T) tensor of integers\n",
        "    returns logits of (B,T,C) (C is channel, here it's same as emb_dim)\n",
        "    \"\"\"\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx) # (B,T,C=emb_dim)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "    x = tok_emb + pos_emb # (B,T,C)\n",
        "    x = self.blocks(x) # (B,T,C)\n",
        "    x = self.ln_f(x) # (B,T,C)\n",
        "\n",
        "    logits = self.lm_head(x) # (B,T,C=vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      # reshape to fit into the torch's cross_entropy function\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T) # or -1\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"idx is (B,T) array of vocab indices in current context\n",
        "    max_new_tokens - number of tokens to generate\n",
        "    return a list of indices of the predicted tokens (from vocab)\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      # in case idx is too long, we chop it to the last block_size of tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, _ = self(idx_cond) # ignore the loss since we are in inference\n",
        "      # we are interested in the last token in the time-series, so only work on that\n",
        "      probs = F.softmax(logits[:, -1, :], dim=-1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append the sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print( sum(p.numel() for p in model.parameters())/1e6, \"M parameters.\" )\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "# training\n",
        "def train(max_iters):\n",
        "  for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val datasets\n",
        "    if iter % eval_interval == 0 or iter == max_iters-1:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  torch.save(m.state_dict(), 'shakespear_params.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpQ_xjLlHKH8",
        "outputId": "3aee6b9e-b0cd-47ec-f737-a0bcd76cf9e2",
        "execution": {
          "iopub.status.busy": "2024-09-09T06:50:25.917692Z",
          "iopub.execute_input": "2024-09-09T06:50:25.918116Z",
          "iopub.status.idle": "2024-09-09T06:50:26.113186Z",
          "shell.execute_reply.started": "2024-09-09T06:50:25.918074Z",
          "shell.execute_reply": "2024-09-09T06:50:26.112186Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "10.788929 M parameters.\n",
          "output_type": "stream"
        }
      ],
      "id": "OpQ_xjLlHKH8"
    },
    {
      "cell_type": "code",
      "source": [
        "# break the training loop to multiple smaller loops\n",
        "train(3000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLDKy468J0aV",
        "outputId": "728fd1fe-d5ba-4cca-b8d3-409443118f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "step 0: train loss 4.2607, val loss 4.2597\n\nstep 100: train loss 2.4853, val loss 2.5009\n\nstep 200: train loss 2.4615, val loss 2.4910\n\nstep 300: train loss 2.4461, val loss 2.4674\n\nstep 400: train loss 2.4303, val loss 2.4591\n\nstep 500: train loss 2.4085, val loss 2.4394\n\nstep 600: train loss 2.3881, val loss 2.4176\n\nstep 700: train loss 2.3368, val loss 2.3710\n\nstep 800: train loss 2.2417, val loss 2.2925\n\nstep 900: train loss 2.1717, val loss 2.2355\n\nstep 1000: train loss 2.1064, val loss 2.1934\n\nstep 1100: train loss 2.0322, val loss 2.1299\n\nstep 1200: train loss 1.9551, val loss 2.0734\n\nstep 1300: train loss 1.8858, val loss 2.0221\n\nstep 1400: train loss 1.8097, val loss 1.9566\n\nstep 1500: train loss 1.7608, val loss 1.9230\n\nstep 1600: train loss 1.7185, val loss 1.8918\n\nstep 1700: train loss 1.6713, val loss 1.8346\n\nstep 1800: train loss 1.6420, val loss 1.8138\n\nstep 1900: train loss 1.6104, val loss 1.7894\n\nstep 2000: train loss 1.5839, val loss 1.7573\n\nstep 2100: train loss 1.5597, val loss 1.7397\n\nstep 2200: train loss 1.5497, val loss 1.7348\n\nstep 2300: train loss 1.5269, val loss 1.7100\n\nstep 2400: train loss 1.5076, val loss 1.7005\n\nstep 2500: train loss 1.4906, val loss 1.6952\n\nstep 2600: train loss 1.4725, val loss 1.6696\n\nstep 2700: train loss 1.4582, val loss 1.6605\n\nstep 2800: train loss 1.4528, val loss 1.6535\n\nstep 2900: train loss 1.4371, val loss 1.6415\n\nstep 2999: train loss 1.4254, val loss 1.6334\n"
        }
      ],
      "id": "zLDKy468J0aV"
    },
    {
      "cell_type": "code",
      "source": [
        "# train for another 3000 loops\n",
        "train(3000)"
      ],
      "metadata": {
        "id": "kvsMclNYMyuF",
        "outputId": "64a5feb4-20de-42a5-c6ca-d7c508f4e952",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2024-09-09T06:51:33.385282Z",
          "iopub.execute_input": "2024-09-09T06:51:33.386189Z",
          "iopub.status.idle": "2024-09-09T07:22:20.707657Z",
          "shell.execute_reply.started": "2024-09-09T06:51:33.386136Z",
          "shell.execute_reply": "2024-09-09T07:22:20.706642Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "step 0: train loss 1.4247, val loss 1.6341\nstep 100: train loss 1.4234, val loss 1.6354\nstep 200: train loss 1.4250, val loss 1.6334\nstep 300: train loss 1.4257, val loss 1.6337\nstep 400: train loss 1.4244, val loss 1.6364\nstep 500: train loss 1.4241, val loss 1.6350\nstep 600: train loss 1.4220, val loss 1.6366\nstep 700: train loss 1.4233, val loss 1.6361\nstep 800: train loss 1.4241, val loss 1.6347\nstep 900: train loss 1.4251, val loss 1.6315\nstep 1000: train loss 1.4248, val loss 1.6372\nstep 1100: train loss 1.4259, val loss 1.6313\nstep 1200: train loss 1.4250, val loss 1.6315\nstep 1300: train loss 1.4242, val loss 1.6315\nstep 1400: train loss 1.4234, val loss 1.6312\nstep 1500: train loss 1.4233, val loss 1.6341\nstep 1600: train loss 1.4259, val loss 1.6349\nstep 1700: train loss 1.4244, val loss 1.6356\nstep 1800: train loss 1.4244, val loss 1.6321\nstep 1900: train loss 1.4273, val loss 1.6333\nstep 2000: train loss 1.4250, val loss 1.6325\nstep 2100: train loss 1.4247, val loss 1.6328\nstep 2200: train loss 1.4239, val loss 1.6356\nstep 2300: train loss 1.4240, val loss 1.6367\nstep 2400: train loss 1.4229, val loss 1.6343\nstep 2500: train loss 1.4252, val loss 1.6327\nstep 2600: train loss 1.4253, val loss 1.6311\nstep 2700: train loss 1.4255, val loss 1.6332\nstep 2800: train loss 1.4250, val loss 1.6360\nstep 2900: train loss 1.4234, val loss 1.6330\nstep 2999: train loss 1.4236, val loss 1.6332\n",
          "output_type": "stream"
        }
      ],
      "id": "kvsMclNYMyuF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and inference"
      ],
      "metadata": {
        "id": "pQBWl76E8T_t"
      },
      "id": "pQBWl76E8T_t"
    },
    {
      "cell_type": "code",
      "source": [
        "saved_filename = '../input/gpt2_shakespear/pytorch/default/1/shakespear_params_3000.pth'\n",
        "model = BigramLanguageModel()\n",
        "model.load_state_dict(torch.load(saved_filename))\n",
        "m = model.to(device)\n",
        "\n",
        "# inference\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "l-LlGGO66NeN",
        "outputId": "a058e546-d3ff-4b14-e77b-a62e8524169a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2024-09-09T06:50:42.041146Z",
          "iopub.execute_input": "2024-09-09T06:50:42.041653Z",
          "iopub.status.idle": "2024-09-09T06:51:14.947786Z",
          "shell.execute_reply.started": "2024-09-09T06:50:42.041613Z",
          "shell.execute_reply": "2024-09-09T06:51:14.946863Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_36/3072692993.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(saved_filename))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nSave than again up on, if more the far with your tongue;\nMakes nayw know not was and Catbbroke.\nGEor.\n\nCLARENCE:\nCome, what's love hear desight means,\nTho, I leave wisholy shome I may to royw,\nAnd not all his fallower. Warwick, farewell,\nAnd blundersile me both to Gentlems, Carion,\nThough Comn Hastings would father?\n\nCORIOLANUS:\nKnow lord, lows, And granthing him.\nWhat me, my Signera Willingelanst of him dame of qudet,\nWith her should by rot me.\n\nCORIOLANUS:\nMird!\nThis All he's ress mine honous me.\nBut An lest before presseque' war'd heaven it at,\nA when to his hemit appronabear is, so por gentleman,\nOr sen it it All up: one metimp, facter of bid\nThe prisof friencal which shall roon trabved to hithers suims he\nceving hands honour sunder to strittly.\nANGELO:\nMore 'eward parly in braquestited repith royle.\n\nDUKE VINCENTIO:\nWell, enver the news?\n\nJULIET:\nBoth I wrong poor the tormers of wes the thousand\nTo hither the returph of Marcius seeR: mett cannot found:\nTen, frown did hend dukning drid rutose too;\nThe plaxe in soverne deberfore twan:\nmy we resol tongues' thousand both oratons wosh\nLady tong of her! inement the met-plutted!\n'TLouchtneity!\nOfficerNTHERY BOLKEY:\nYou is me is beon the soul! Veretonge.\n\nMENIUS:\nAll call.\n\nBUCKINGHAM:\nO may was noble well.\n\nVOLUMNIA:\nTo mean at diller:\nHold with other fith our time: so he the\ncrust streng at mo absson\nHe's, in rother obide yeye.\nCarrioh lordshiongs?\n\nSirst Senatolr:\nHeven shall tage\npardorr:\nBetter, did more be one no rotunting?\n\nRICHMORS OF YORK:\nI will not weepty me.\n\nSecondman:\nWhat, that's up on not his for, 'tis thou as to may:\nI privot not the part, in a the lige.\nWhen thou they goodst allias, or beith's for-hee w,\nYou at all wears accesed, made so eyers,\nA we packs thou preent your such Saider consper'd: most los first:\nI sratight not tin madam mock of Lone Gaunther\nTo breast my forbids to bear sun, and irtous flang,\nAnd her away forcuption friends up me;\nSo, sintand our be a slafe weell, thou thy hate\nIs to so\n",
          "output_type": "stream"
        }
      ],
      "id": "l-LlGGO66NeN"
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-09T07:24:14.683953Z",
          "iopub.execute_input": "2024-09-09T07:24:14.684387Z",
          "iopub.status.idle": "2024-09-09T07:24:46.761386Z",
          "shell.execute_reply.started": "2024-09-09T07:24:14.684347Z",
          "shell.execute_reply": "2024-09-09T07:24:46.760465Z"
        },
        "trusted": true,
        "id": "GanZQS3H5EYn",
        "outputId": "ec7089cc-4175-46bb-d05d-f8a97113c72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nTis sweepe no alonef.\n\nDUKE OF YORK:\nRunnight! Norses my besset and our short,\nCamoher headrless, sove as as truest, and\nempentunce the goove, for Rathage one behing misch\nthe trotom a morrus, I was a love off hath\nTo death make of be be ouragus anys gatin a shall:\nnot tut unthat Cusin: I help time our bid,\nAnd but vaniam to thee, I day mustorn:\nCheesbel, none of none Anainst of mine!\nResoon, I know but her him too rather's gone.\nWhy chosine in my quest for, first comme.\n\nFRIAR LAURENCE:\nAun strew he heaven all in I cage bee within\nFor my so with me, ground Keesting on Rives intly BaR.\nI prive shame FRICHARD:\nGod slave news ut, add here high as the death?\nI will: Rancaus, it that hath honourset;\nWith his kinsming, and in the what star's his;\nThe own from the dansethling up\nar of my trespeak'd: retther of brother they\nvanious of my state, whereof prouse thee! for to hate.\n\nCATISUS:\nNot sould with not yourselves thee honess\nEvongs by dill-letting myst it:\nAnd poor be me now the gentler eyes,\nI not windemy this be have need,--ade fiftne'd:\nConter on speaking away's for we in.\n\nCORIOLANUS:\nYourseds chat for you wouh in Landoment Mono.\n\nFirst Down:\nPreviol Lord Capter:\nFor be fleding, nay to dight to Tybreward:\nAway, let the pail the sendly my dier wrown I.\nStill, it what not its virtume\nThan for them talk us a bring agaginster:\nTo giving, ay, only by guing out\nBhittly to weshinks lieft the matched. The never,\nTo morrabs and becomey heave have-bard!\nForr, convitam and runs, one perison or debes,\nAnd, will these rog chold had bald soldable on. What\nleaves the her, I cannot comfe whuld stop?\n\nROMEO:\nWho'st unto subs here too thest which forth,\nWith kindingders wrogh deest:\nThere his hand wars.\n'Hom, that doth not when little. I preved her blast.\nMist God won stillip advery nugh. Be to therse,\nTo eyes RinCn:\nComillown: say help you under hear,\nSee no there den leave out stators in forbs me her,\nAs frome and die you lover madness parar casurs\nAs is, and heavy is not old aten\n",
          "output_type": "stream"
        }
      ],
      "id": "GanZQS3H5EYn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference result when loss was 1.4-1.6:\n",
        "<pre>\n",
        "She will  grown of Stices\n",
        "Thre Pricius there her issue to'er, it hence,\n",
        "Firsth out helpt on her designg\n",
        "There all liverend ught be ring he shall\n",
        "I the movenward the graves person of afiry:\n",
        "Gold my in: wilthor, may on' this be heaven at day,\n",
        "Sraigh'd the premut cave of Prons,\n",
        "Yet temptinate,\n",
        "Hag their rablic surp ourselved obey. God our him,\n",
        "A cove thou wneeded hast his worship; here, whose would have,\n",
        "dels bars upase, this Hensmis my I do your mine.\n",
        "A were had is have yours camrcibled me sleapn'd equiet,\n",
        "Twes can I she causled. Brink all I companirfullouds,\n",
        "tre knows known wray in inteor,\n",
        "his her, and that make sust wouldst to stays,\n",
        "To a ndeed for that viole with me pennizents of my life:\n",
        "True not.\n",
        "\n",
        "NORTURS:\n",
        "And, I do love have not, must give thoughn'd rell, west,\n",
        "Inatheron what command out day the four thee\n",
        "hr if thing away troward droust: then fall them you.\n",
        "\n",
        "BUCOLIO:\n",
        "I'll know to hereb'd of your roficining'd,\n",
        "As what give woful king Bengeo, his love.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "Not with very with hon, tell from fear. Richmond;\n",
        "See Marcis of York, whilh stragg it if after,\n",
        "With fialin I yours lord my Lord Margun with his should wonds,\n",
        "Mittle most to be what his proud desped's\n",
        "And rise good but his love, doth not plud not, brille.\n",
        "Why will ministining Adlow of yours thands.\n",
        "\n",
        "HASTRINGS:\n",
        "This do be to the wis dance in the bear.\n",
        "Your, in with the mod boh that I mean him know,\n",
        "hath in Is pard abuneled himsent on, his\n",
        "make that's makes was I have and be prayed;\n",
        "Annor me not common near be confotsce of andel,\n",
        "But be a fesil strange, and to re,\n",
        "But his out loy: well, sing the make carged on such toge.\n",
        "\n",
        "NORFOok, fasagon, Sir Gavet, stand thosows stong he\n",
        "reash yours our countrupten, have strrow; let me\n",
        "Engle midnet thou Rown me slew up, not orden hence\n",
        "To tell and a I hap,\n",
        "Her not fater' pluke herd so I have revint degreate.\n",
        "\n",
        "CAMIDIUS:\n",
        "So, all a we reporer the plect me: and beher shall'd\n",
        "Our hyrnight.\n",
        "\n",
        "BARNARD:\n",
        "O surCUS:\n",
        "Let whom he the was a too desteritions.\n",
        "\n",
        "3EDWARD PARDISS VI\n",
        "</pre>"
      ],
      "metadata": {
        "id": "2ql_eC9fY3Gy"
      },
      "id": "2ql_eC9fY3Gy"
    }
  ]
}