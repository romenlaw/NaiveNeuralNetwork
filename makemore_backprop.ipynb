{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romenlaw/NaiveNeuralNetwork/blob/main/makemore_backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4f4JG1gdKqj"
      },
      "source": [
        "#Makemore - backprop ninja"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prepare datasets"
      ],
      "metadata": {
        "id": "vTLLwG6T_Srt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/romenlaw/NaiveNeuralNetwork/main/names.txt"
      ],
      "metadata": {
        "id": "WnJ_g9N870O8",
        "outputId": "274ca161-bf0b-4a2c-b904-c65a0ed5bbd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  222k  100  222k    0     0   431k      0 --:--:-- --:--:-- --:--:--  431k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "kI-rQ1qpCWoV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n",
        "len(words), max(len(w) for w in words), words[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RRwKoRN_V0p",
        "outputId": "728a416d-7961-4bb9-834e-8460b66fb7a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32033,\n",
              " 15,\n",
              " ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(list(set(''.join(words))))\n",
        "vocab.insert(0, '.')\n",
        "itos = { i:s for i,s in enumerate(vocab)}\n",
        "stoi = { s:i for i,s in enumerate(vocab)}\n",
        "vocab_size = len(vocab)  # 27"
      ],
      "metadata": {
        "id": "rWeh-qTNAOxO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 3  # context size - 3 tokens\n",
        "\n",
        "def build_dataset(words):\n",
        "  \"\"\"returns torch tensors X, Y where\n",
        "  X is a list of n-grams indices covering the whole words list, where n=block_size\n",
        "  Y is a list of indices predicting each n-gram in X\n",
        "  \"\"\"\n",
        "  X, Y = [], []\n",
        "\n",
        "  #for w in words[:5]:\n",
        "  for w in words:\n",
        "    context = [0] * block_size # repeat '.' to fill block_size\n",
        "    for ch in w+'.':\n",
        "      ix = stoi[ch]\n",
        "      #print(' '.join([itos[i] for i in context]), '---->', itos[ix])\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix]\n",
        "\n",
        "  return torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "X, Y = build_dataset(words)\n",
        "#X[:32], Y[:32]\n",
        "\n",
        "# split the data into 3 sets\n",
        "# 80% for training set\n",
        "# 10% for validation/development\n",
        "# 10% for testing\n",
        "import random\n",
        "random.seed(42)\n",
        "n1 = int(len(words) * .8)\n",
        "n2 = int(len(words) * .9)\n",
        "random.shuffle(words) # shuffle is in-place\n",
        "X_train, Y_train = build_dataset(words[:n1])\n",
        "X_dev, Y_dev = build_dataset(words[n1:n2])\n",
        "X_test, Y_test = build_dataset(words[n2:])\n",
        "\n",
        "#len(words[n1:n2])\n",
        "(X_train.shape, Y_train.shape), (X_dev.shape, Y_dev.shape), (X_test.shape, Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_wuMWU1BUFe",
        "outputId": "42dd9400-fc9c-49b3-c58b-f22346152f73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((torch.Size([182625, 3]), torch.Size([182625])),\n",
              " (torch.Size([22655, 3]), torch.Size([22655])),\n",
              " (torch.Size([22866, 3]), torch.Size([22866])))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utilities"
      ],
      "metadata": {
        "id": "_Zrl2yoKDHSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utility to compare our manual gradients with pytorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  \"\"\"Compares dt and t.grad to see if their values are equal or close\n",
        "  s - name of the parameter being compared, used in printing only\n",
        "  dt - tensor of manually calculated gradient\n",
        "  t - torch tensor\n",
        "  \"\"\"\n",
        "  ex = torch.all(dt==t.grad).item()\n",
        "  apx = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approx: {str(apx):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "x9d-letYDPF7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP\n",
        "In the parameter initialisation, we use non-standard values to see their effect; otherwise, for example, zeros can mask out any incorrect values."
      ],
      "metadata": {
        "id": "VAOQhg9HNNyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 10\n",
        "hidden_dim = 200\n",
        "\n",
        "g = torch.Generator().manual_seed(20240824)\n",
        "C = torch.randn((vocab_size, embed_dim),  generator=g)\n",
        "\n",
        "# hidden layer\n",
        "fan_in = embed_dim*block_size # we concat multiple C's to feed into hidden layer\n",
        "W1 = torch.randn((fan_in, hidden_dim), generator=g) * (5/3 / fan_in**0.5)\n",
        "b1 = torch.randn(hidden_dim,           generator=g) * 0.1 # experiment\n",
        "# output layer\n",
        "W2 = torch.randn((hidden_dim, vocab_size), generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,               generator=g) * 0.1 # experiment with non-zero\n",
        "\n",
        "# batch normalisation 1D layer placed after hidden layer, hence dim=hidden_dim\n",
        "bn_gamma = torch.randn((1, hidden_dim),    generator=g) * 0.1 + 1.0\n",
        "bn_bias = torch.randn((1, hidden_dim),     generator=g) * 0.1\n",
        "#bn_running_mean = torch.zeros((1, hidden_dim))\n",
        "#bn_running_std = torch.ones((1, hidden_dim))\n",
        "\n",
        "# the above are initialised with non-standard values to magnify any incorrect values\n",
        "\n",
        "parameters = [C, W1, W2, b2, bn_gamma, bn_bias]\n",
        "print('total params: ', sum([p.nelement() for p in parameters]))\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4EIrTkZNPpo",
        "outputId": "1c2774b5-7042-41d4-cd5d-7e69aeb9d7f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total params:  12097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training - extended version\n",
        "We expand the forward pass into step by step calculations so that we can manually calculate the gradient step by step as well. For Cross Entropy loss function, see [pyTorch doco](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss).\n",
        "\n",
        "We don't call the loss.backward(). Instead, we will do it manually."
      ],
      "metadata": {
        "id": "T9c-gp2PM-Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# understanding tensor.values, which only works on sparse tensor\n",
        "t = torch.randn((2,3))\n",
        "sparse_tensor = t.max(dim=1, keepdim=True)\n",
        "t, '-----------', sparse_tensor, '------------', sparse_tensor.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4UqInlQLqw4",
        "outputId": "1ceb2f4b-1b1d-4aad-ab35-7ede494d677d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-2.0463, -0.0297,  1.5362],\n",
              "         [ 0.7847,  0.8150,  0.7719]]),\n",
              " '-----------',\n",
              " torch.return_types.max(\n",
              " values=tensor([[1.5362],\n",
              "         [0.8150]]),\n",
              " indices=tensor([[2],\n",
              "         [1]])),\n",
              " '------------',\n",
              " tensor([[1.5362],\n",
              "         [0.8150]]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # construct mini-batch:\n",
        "  # generate a list of random indices, length of list if batch_size\n",
        "  ix = torch.randint(low=0, high=X_train.shape[0], size=(batch_size,), generator=g)\n",
        "  xs = X_train[ix]  # (batch_size, block_size)\n",
        "  ys = Y_train[ix]  # (batch_size)\n",
        "\n",
        "  ##################################\n",
        "  # forward pass (expanded version)\n",
        "  ##################################\n",
        "  # embedding ---------------------------\n",
        "  emb = C[xs] # (batch_size, block_size, hidden_dim)\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  # hidden layer ------------------------\n",
        "  h_prebn = embcat @ W1 + b1 # (batch_size, hidden_dim)\n",
        "  # BN layer (expended version) ----------------------------\n",
        "  #bn_mean = h_prebn.mean(dim=0, keepdim=True)\n",
        "  bn_mean = 1/batch_size*h_prebn.sum(dim=0, keepdim=True)\n",
        "  #bn_std = h_prebn.std(dim=0, keepdim=True)\n",
        "  bn_diff = (h_prebn - bn_mean)\n",
        "  bn_diff2 = bn_diff ** 2\n",
        "  bn_var = 1/(batch_size-1) * bn_diff.sum(dim=0, keepdim=True) # Bessel's correction, divide by m-1 not m\n",
        "  bn_varinv = (bn_var + 1e-5)**-0.5  # 1/sqrt(var+eps)\n",
        "  x_hat = bn_diff * bn_varinv\n",
        "  h_preact = bn_gamma * x_hat + bn_bias\n",
        "  #with torch.no_grad():\n",
        "  #  bn_running_mean = 0.999 * bn_running_mean + 0.001 * bn_mean\n",
        "  #  bn_running_std = 0.999 * bn_running_std + 0.001 * bn_std\n",
        "  # Non-linearity ----------------------\n",
        "  h = torch.tanh(h_preact)  # (batch_size, hidden_dim)\n",
        "  # output layer -----------------------\n",
        "  logits = h @ W2 + b2 # (hidden_dim, vocab_size)\n",
        "  # loss function (extended version) ----------------------\n",
        "  #loss = F.cross_entropy(logits, ys)\n",
        "  logit_maxes = logits.max(dim=1, keepdim=True).values  # (hidden_dim, 1)\n",
        "  norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "  counts = norm_logits.exp()  # (batch_size, vocab_size)\n",
        "  counts_sum = counts.sum(dim=1, keepdim=True)\n",
        "  counts_sum_inv = counts_sum ** -1\n",
        "  probs = counts * counts_sum_inv\n",
        "  logprobs = probs.log()   # (batch_size, vocab_size)\n",
        "  loss = -logprobs[range(batch_size), ys].mean()  # scalar\n",
        "\n",
        "  ################\n",
        "  # backward pass\n",
        "  ################\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  for t in [logprobs, probs, counts_sum_inv, counts_sum, counts,\n",
        "            norm_logits, logit_maxes, logits,\n",
        "            h, h_preact, x_hat, bn_varinv, bn_var, bn_diff2, bn_diff, bn_mean,\n",
        "            h_prebn, embcat, emb ]:\n",
        "    t.retain_grad()\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  ###############\n",
        "  # update\n",
        "  ###############\n",
        "  # lr = 0.1 if i<100000 else 0.01\n",
        "  # for p in parameters:\n",
        "  #   p.data += -lr * p.grad\n",
        "\n",
        "  # # tracking\n",
        "  # lossi.append(loss)\n",
        "  # if i%10000 == 0:\n",
        "  #   print('%6d/%7d %2.10f' % (i, max_steps, loss))\n",
        "\n",
        "  # if i>1000:\n",
        "  break\n",
        "print('%6d/%7d %2.10f' % (i, max_steps, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOpGCG2TNAkI",
        "outputId": "e9a92f01-6958-4939-c88d-26dfbf46d8da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     0/ 200000 4.0160999298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ys.shape"
      ],
      "metadata": {
        "id": "Wb19DCBUFD3W",
        "outputId": "42ed2cd3-13dc-44e0-aefb-ca4c5327088f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1 - backward pass"
      ],
      "metadata": {
        "id": "ry8Xbe9-DPNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dlogprobs\n",
        "logprobs is dimension (N, vocab_size) , where N = batch_size\n",
        "$$loss = -\\dfrac{1}{N}\\sum_{i=1}^{N}logprobs_{i, y_i}$$\n",
        "$$\\dfrac{\\delta loss}{\\delta logprobs}=\n",
        "\\begin{cases}\n",
        "-\\dfrac{1}{N} & \\quad \\text{at positions }{i},{y_i}\\\\\n",
        "0 & \\quad \\text{elsewhere}\n",
        "\\end{cases}\n",
        "$$"
      ],
      "metadata": {
        "id": "zcMTxreLHKO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(batch_size), ys] = -1/batch_size\n",
        "cmp('dlogprobs', dlogprobs, logprobs)"
      ],
      "metadata": {
        "id": "ek7Mmk4fO9kt",
        "outputId": "f09a863c-cae4-4bc1-d901-52a26bbe9ddc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dlogprobs       | exact: True  | approx: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dlogprobs\n",
        "$$logprobs = \\ln(probs)$$\n",
        "$$\\dfrac{\\delta logprobs}{\\delta probs} = \\dfrac{1}{probs}\n",
        "$$"
      ],
      "metadata": {
        "id": "i_6hGt3IJduM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dlogprobs = 1/probs # omit the *dloss since dloss=1"
      ],
      "metadata": {
        "id": "4ZAWM-h4T_3y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmp('dlogprobs', dlogprobs, logprobs)\n",
        "dlogprobs.shape"
      ],
      "metadata": {
        "id": "qUwrnaS7KURu",
        "outputId": "dfc8f609-e1dd-49d3-f900-b5e0057904fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dlogprobs       | exact: False | approx: False | maxdiff: 3598.58984375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "probs[0], logprobs[0], math.log(0.0066)"
      ],
      "metadata": {
        "id": "y2VR1zGJKonc",
        "outputId": "72c24777-bb8a-4728-f643-29fee4598073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.0171, 0.0066, 0.0091, 0.0543, 0.0534, 0.0135, 0.0023, 0.2083, 0.0601,\n",
              "         0.1704, 0.0005, 0.0032, 0.0210, 0.0032, 0.0233, 0.1039, 0.0073, 0.0038,\n",
              "         0.0319, 0.0212, 0.0172, 0.0036, 0.0025, 0.0191, 0.1093, 0.0230, 0.0110],\n",
              "        grad_fn=<SelectBackward0>),\n",
              " tensor([-4.0706, -5.0247, -4.7035, -2.9132, -2.9308, -4.3018, -6.0565, -1.5687,\n",
              "         -2.8119, -1.7698, -7.5795, -5.7603, -3.8640, -5.7566, -3.7590, -2.2639,\n",
              "         -4.9168, -5.5704, -3.4465, -3.8535, -4.0607, -5.6183, -5.9743, -3.9600,\n",
              "         -2.2140, -3.7742, -4.5074], grad_fn=<SelectBackward0>),\n",
              " -5.0206856299497575)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logprobs.grad[0], dlogprobs[0]"
      ],
      "metadata": {
        "id": "-y8HNLAhLnl9",
        "outputId": "25c0d904-105e-461d-c09d-36a621b14274",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000, -0.0312,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000]),\n",
              " tensor([  58.5935,  152.1314,  110.3359,   18.4162,   18.7425,   73.8336,\n",
              "          426.8828,    4.8003,   16.6409,    5.8695, 1957.6235,  317.4359,\n",
              "           47.6570,  316.2625,   42.9069,    9.6209,  136.5681,  262.5268,\n",
              "           31.3899,   47.1599,   58.0156,  275.4112,  393.1922,   52.4590,\n",
              "            9.1519,   43.5641,   90.6864], grad_fn=<SelectBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "makemore_backprop.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}