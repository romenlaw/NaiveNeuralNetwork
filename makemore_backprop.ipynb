{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romenlaw/NaiveNeuralNetwork/blob/main/makemore_backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4f4JG1gdKqj"
      },
      "source": [
        "#Makemore - backprop ninja"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prepare datasets"
      ],
      "metadata": {
        "id": "vTLLwG6T_Srt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/romenlaw/NaiveNeuralNetwork/main/names.txt"
      ],
      "metadata": {
        "id": "WnJ_g9N870O8",
        "outputId": "4b203660-1a46-46b3-8053-d2e99b693f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  222k  100  222k    0     0  1120k      0 --:--:-- --:--:-- --:--:-- 1125k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "kI-rQ1qpCWoV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n",
        "len(words), max(len(w) for w in words), words[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RRwKoRN_V0p",
        "outputId": "87e67b5a-b331-4146-a1a4-1bc931c90159"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32033,\n",
              " 15,\n",
              " ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(list(set(''.join(words))))\n",
        "vocab.insert(0, '.')\n",
        "itos = { i:s for i,s in enumerate(vocab)}\n",
        "stoi = { s:i for i,s in enumerate(vocab)}\n",
        "vocab_size = len(vocab)  # 27"
      ],
      "metadata": {
        "id": "rWeh-qTNAOxO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 3  # context size - 3 tokens\n",
        "\n",
        "def build_dataset(words):\n",
        "  \"\"\"returns torch tensors X, Y where\n",
        "  X is a list of n-grams indices covering the whole words list, where n=block_size\n",
        "  Y is a list of indices predicting each n-gram in X\n",
        "  \"\"\"\n",
        "  X, Y = [], []\n",
        "\n",
        "  #for w in words[:5]:\n",
        "  for w in words:\n",
        "    context = [0] * block_size # repeat '.' to fill block_size\n",
        "    for ch in w+'.':\n",
        "      ix = stoi[ch]\n",
        "      #print(' '.join([itos[i] for i in context]), '---->', itos[ix])\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix]\n",
        "\n",
        "  return torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "X, Y = build_dataset(words)\n",
        "#X[:32], Y[:32]\n",
        "\n",
        "# split the data into 3 sets\n",
        "# 80% for training set\n",
        "# 10% for validation/development\n",
        "# 10% for testing\n",
        "import random\n",
        "random.seed(42)\n",
        "n1 = int(len(words) * .8)\n",
        "n2 = int(len(words) * .9)\n",
        "random.shuffle(words) # shuffle is in-place\n",
        "X_train, Y_train = build_dataset(words[:n1])\n",
        "X_dev, Y_dev = build_dataset(words[n1:n2])\n",
        "X_test, Y_test = build_dataset(words[n2:])\n",
        "\n",
        "#len(words[n1:n2])\n",
        "(X_train.shape, Y_train.shape), (X_dev.shape, Y_dev.shape), (X_test.shape, Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_wuMWU1BUFe",
        "outputId": "47e42f7e-be56-466d-a726-51ef6573185d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((torch.Size([182625, 3]), torch.Size([182625])),\n",
              " (torch.Size([22655, 3]), torch.Size([22655])),\n",
              " (torch.Size([22866, 3]), torch.Size([22866])))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utilities"
      ],
      "metadata": {
        "id": "_Zrl2yoKDHSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utility to compare our manual gradients with pytorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  \"\"\"Compares dt and t.grad to see if their values are equal or close\n",
        "  s - name of the parameter being compared, used in printing only\n",
        "  dt - tensor of manually calculated gradient\n",
        "  t - torch tensor\n",
        "  \"\"\"\n",
        "  ex = torch.all(dt==t.grad).item()\n",
        "  apx = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approx: {str(apx):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "x9d-letYDPF7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP\n",
        "In the parameter initialisation, we use non-standard values to see their effect; otherwise, for example, zeros can mask out any incorrect values."
      ],
      "metadata": {
        "id": "VAOQhg9HNNyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 10\n",
        "hidden_dim = 200\n",
        "\n",
        "g = torch.Generator().manual_seed(20240824)\n",
        "C = torch.randn((vocab_size, embed_dim),  generator=g)\n",
        "\n",
        "# hidden layer\n",
        "fan_in = embed_dim*block_size # we concat multiple C's to feed into hidden layer\n",
        "W1 = torch.randn((fan_in, hidden_dim), generator=g) * (5/3 / fan_in**0.5)\n",
        "b1 = torch.randn(hidden_dim,           generator=g) * 0.1 # experiment\n",
        "# output layer\n",
        "W2 = torch.randn((hidden_dim, vocab_size), generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,               generator=g) * 0.1 # experiment with non-zero\n",
        "\n",
        "# batch normalisation 1D layer placed after hidden layer, hence dim=hidden_dim\n",
        "bn_gamma = torch.randn((1, hidden_dim),    generator=g) * 0.1 + 1.0\n",
        "bn_bias = torch.randn((1, hidden_dim),     generator=g) * 0.1\n",
        "#bn_running_mean = torch.zeros((1, hidden_dim))\n",
        "#bn_running_std = torch.ones((1, hidden_dim))\n",
        "\n",
        "# the above are initialised with non-standard values to magnify any incorrect values\n",
        "\n",
        "parameters = [C, W1, W2, b2, bn_gamma, bn_bias]\n",
        "print('total params: ', sum([p.nelement() for p in parameters]))\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4EIrTkZNPpo",
        "outputId": "3d697957-fecd-4981-ec31-9e8648d76ab8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total params:  12097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training - extended version\n",
        "We expand the forward pass into step by step calculations so that we can manually calculate the gradient step by step as well. For Cross Entropy loss function, see [pyTorch doco](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss).\n",
        "\n",
        "We don't call the loss.backward(). Instead, we will do it manually."
      ],
      "metadata": {
        "id": "T9c-gp2PM-Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# understanding tensor.values, which only works on sparse tensor\n",
        "t = torch.randn((2,3))\n",
        "sparse_tensor = t.max(dim=1, keepdim=True)\n",
        "t, '-----------', sparse_tensor, '------------', sparse_tensor.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4UqInlQLqw4",
        "outputId": "222325d2-1251-4e2f-fdff-4c8cf0afdb0c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.8586, -0.6088,  0.6713],\n",
              "         [ 0.1186, -2.1520,  0.7360]]),\n",
              " '-----------',\n",
              " torch.return_types.max(\n",
              " values=tensor([[0.6713],\n",
              "         [0.7360]]),\n",
              " indices=tensor([[2],\n",
              "         [2]])),\n",
              " '------------',\n",
              " tensor([[0.6713],\n",
              "         [0.7360]]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # construct mini-batch:\n",
        "  # generate a list of random indices, length of list if batch_size\n",
        "  ix = torch.randint(low=0, high=X_train.shape[0], size=(batch_size,), generator=g)\n",
        "  xs = X_train[ix]  # (batch_size, block_size)\n",
        "  ys = Y_train[ix]  # (batch_size)\n",
        "\n",
        "  ##################################\n",
        "  # forward pass (expanded version)\n",
        "  ##################################\n",
        "  # embedding ---------------------------\n",
        "  emb = C[xs] # (batch_size, block_size, hidden_dim)\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  # hidden layer ------------------------\n",
        "  h_prebn = embcat @ W1 + b1 # (batch_size, hidden_dim)\n",
        "  # BN layer (expended version) ----------------------------\n",
        "  #bn_mean = h_prebn.mean(dim=0, keepdim=True)\n",
        "  bn_mean = 1/batch_size*h_prebn.sum(dim=0, keepdim=True)\n",
        "  #bn_std = h_prebn.std(dim=0, keepdim=True)\n",
        "  bn_diff = (h_prebn - bn_mean)\n",
        "  bn_diff2 = bn_diff ** 2\n",
        "  bn_var = 1/(batch_size-1) * bn_diff.sum(dim=0, keepdim=True) # Bessel's correction, divide by m-1 not m\n",
        "  bn_varinv = (bn_var + 1e-5)**-0.5  # 1/sqrt(var+eps)\n",
        "  x_hat = bn_diff * bn_varinv\n",
        "  h_preact = bn_gamma * x_hat + bn_bias\n",
        "  #with torch.no_grad():\n",
        "  #  bn_running_mean = 0.999 * bn_running_mean + 0.001 * bn_mean\n",
        "  #  bn_running_std = 0.999 * bn_running_std + 0.001 * bn_std\n",
        "  # Non-linearity ----------------------\n",
        "  h = torch.tanh(h_preact)  # (batch_size, hidden_dim)\n",
        "  # output layer -----------------------\n",
        "  logits = h @ W2 + b2 # (hidden_dim, vocab_size)\n",
        "  # loss function (extended version) ----------------------\n",
        "  #loss = F.cross_entropy(logits, ys)\n",
        "  logit_maxes = logits.max(dim=1, keepdim=True).values  # (hidden_dim, 1)\n",
        "  norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "  counts = norm_logits.exp()  # (batch_size, vocab_size)\n",
        "  counts_sum = counts.sum(dim=1, keepdim=True) # (batch_size, 1)\n",
        "  counts_sum_inv = counts_sum ** -1  # (batch_size, 1)\n",
        "  probs = counts * counts_sum_inv  # (batch_size, vocab_size)\n",
        "  logprobs = probs.log()   # (batch_size, vocab_size)\n",
        "  loss = -logprobs[range(batch_size), ys].mean()  # scalar\n",
        "\n",
        "  ################\n",
        "  # backward pass\n",
        "  ################\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  for t in [logprobs, probs, counts_sum_inv, counts_sum, counts,\n",
        "            norm_logits, logit_maxes, logits,\n",
        "            h, h_preact, x_hat, bn_varinv, bn_var, bn_diff2, bn_diff, bn_mean,\n",
        "            h_prebn, embcat, emb ]:\n",
        "    t.retain_grad()\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  ###############\n",
        "  # update\n",
        "  ###############\n",
        "  # lr = 0.1 if i<100000 else 0.01\n",
        "  # for p in parameters:\n",
        "  #   p.data += -lr * p.grad\n",
        "\n",
        "  # # tracking\n",
        "  # lossi.append(loss)\n",
        "  # if i%10000 == 0:\n",
        "  #   print('%6d/%7d %2.10f' % (i, max_steps, loss))\n",
        "\n",
        "  # if i>1000:\n",
        "  break\n",
        "print('%6d/%7d %2.10f' % (i, max_steps, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOpGCG2TNAkI",
        "outputId": "92c9b366-d03f-42df-c0aa-781dedd446d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     0/ 200000 4.0160999298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ys.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb19DCBUFD3W",
        "outputId": "42ed2cd3-13dc-44e0-aefb-ca4c5327088f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - backward pass of loss function"
      ],
      "metadata": {
        "id": "ry8Xbe9-DPNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dlogprobs\n",
        "logprobs is dimension (N, vocab_size) , where N = batch_size\n",
        "$$loss = -\\dfrac{1}{N}\\sum_{i=1}^{N}logprobs_{i, y_i}$$\n",
        "For the loss function, only elements at indices $[i, y_i]$ contribute to the loss. The rest are not used. Therefore, the unused elements have gradient 0.\n",
        "$$\\dfrac{\\delta loss}{\\delta logprobs}=\n",
        "\\begin{cases}\n",
        "-\\dfrac{1}{N} & \\quad \\text{at positions }{i},{y_i}\\\\\n",
        "0 & \\quad \\text{elsewhere}\n",
        "\\end{cases}\n",
        "$$"
      ],
      "metadata": {
        "id": "zcMTxreLHKO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(batch_size), ys] = -1/batch_size\n",
        "cmp('dlogprobs', dlogprobs, logprobs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek7Mmk4fO9kt",
        "outputId": "b205ee4e-4ddc-4d8d-af5f-48f78766d242"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dlogprobs       | exact: True  | approx: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dprobs\n",
        "$$logprobs = \\ln(probs)$$\n",
        "$$\\dfrac{\\delta logprobs}{\\delta probs} = \\dfrac{1}{probs}\n",
        "$$"
      ],
      "metadata": {
        "id": "i_6hGt3IJduM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dprobs = 1/probs * dlogprobs\n",
        "cmp('dprobs', dprobs, probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZAWM-h4T_3y",
        "outputId": "f377bb45-d27f-4c33-c9d1-f7aa3795ef62"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dprobs          | exact: True  | approx: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dcount_sum_inv\n",
        "\n",
        "$$probs = counts * \\text{counts_sum_inv}\n",
        "$$\n",
        "$$\\dfrac{\\delta probs}{\\delta \\text{counts_sum_inv}} = counts\n",
        "$$\n",
        "Note that the dimension of counts_sum_inv is (N, 1), so does its derivative."
      ],
      "metadata": {
        "id": "ocS5fuREeaw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dcounts_sum_inv = counts * dprobs\n",
        "dcounts_sum_inv = dcounts_sum_inv.sum(dim=1, keepdim=True)\n",
        "cmp('dcounts_sum_inv', dcounts_sum_inv, counts_sum_inv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUwrnaS7KURu",
        "outputId": "a7751dc2-4bbd-4e76-b289-640475e72109"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dcounts_sum_inv | exact: True  | approx: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dcounts_sum\n",
        "$$\\text{counts_sum_inv} = \\text{counts_sum}^{-1}\n",
        "$$\n",
        "$$\\dfrac{\\delta\\text{counts_sum_inv}}{\\delta\\text{counts_sum}} = -\\text{counts_sum}^{-2}\n",
        "$$"
      ],
      "metadata": {
        "id": "91_4MFMKh1aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dcounts_sum=-counts_sum**-2\n",
        "dcounts_sum *= dcounts_sum_inv\n",
        "cmp('dcounts_sum', dcounts_sum, counts_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2VR1zGJKonc",
        "outputId": "394197ad-c641-4d8b-f092-290597a20c2a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dcounts_sum     | exact: True  | approx: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dcounts_sum[0], counts.shape"
      ],
      "metadata": {
        "id": "UY4dWJRcvU8T",
        "outputId": "2eb3bf7d-5a64-4066-947f-c595640dbc00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.0065], grad_fn=<SelectBackward0>), torch.Size([32, 27]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dcounts\n",
        "counts is used twice in the forward pass: once in counts_sum, another in probs.\n",
        "for counts_sum:\n",
        "$$\\text{counts_sum} = \\sum_{j=1}^{\\text{vocab_size}}counts_{i,j}\n",
        "$$\n",
        "$$\\dfrac{\\delta \\text{counts_sum}}{\\delta counts} = 1_{N, \\text{vocab_size}}\n",
        "$$\n",
        "Note that the 1 is same dimension as counts, i.e. (N, vocab_size)\n",
        "\n",
        "For probs:\n",
        "$$probs = counts * \\text{counts_sum_inv}\n",
        "$$\n",
        "$$\\dfrac{\\delta probs}{\\delta counts} = \\text{counts_sum_inv}\n",
        "$$"
      ],
      "metadata": {
        "id": "68bhFVV7i2n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dcounts1 = torch.ones_like(counts) * dcounts_sum\n",
        "dcounts2 = counts_sum_inv * dprobs\n",
        "dcounts = dcounts1 + dcounts2\n",
        "cmp('dcounts', dcounts, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y8HNLAhLnl9",
        "outputId": "862e28e5-9540-4d8f-fa58-56d61079f284"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dcounts         | exact: True  | approx: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dnorm_logits\n",
        "$$counts = e^{\\text{norm_logits}}\n",
        "$$\n",
        "$$\\dfrac{\\delta counts}{\\delta \\text{norm_logits}} = e^{\\text{norm_logits}}\n",
        "$$"
      ],
      "metadata": {
        "id": "uuwxEfgywi2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dnorm_logits = counts * dcounts\n",
        "cmp('dnorm_logits', dnorm_logits, norm_logits)"
      ],
      "metadata": {
        "id": "IVKWhVSRwlZo",
        "outputId": "d7a5a329-a9e3-4257-fda8-dd4eac5d3201",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dnorm_logits    | exact: True  | approx: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norm_logits.shape"
      ],
      "metadata": {
        "id": "9ZkwOMb_4hG7",
        "outputId": "0fe2c37f-cc97-43ef-dfd3-466cc73fb669",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dlogit_maxes\n",
        "$$\\text{norm_logits}=logits_i - \\text{logit_maxes}_i \\quad \\text{for }i \\in [0,N)\n",
        "$$\n",
        "The dimension of logit_maxes is (N,1)\n",
        "\n",
        "$$\\dfrac{\\delta \\text{norm_logits}}{\\delta \\text{logit_maxes}} = -\\sum_{j}^{\\text{vocab_size}}1\n",
        "$$"
      ],
      "metadata": {
        "id": "QR5zcUsMypVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dlogit_maxes = -dnorm_logits\n",
        "dlogit_maxes = dlogit_maxes.sum(dim=1, keepdim=True)\n",
        "cmp('dlogit_maxes', dlogit_maxes, logit_maxes)"
      ],
      "metadata": {
        "id": "_s6uXNI_0-4l",
        "outputId": "4a4e4bcb-1645-43bd-dfae-a41d6659b8d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dlogit_maxes    | exact: True  | approx: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logit_maxes.shape, dlogit_maxes.shape, logit_maxes.grad[0], dlogit_maxes[0].sum()"
      ],
      "metadata": {
        "id": "htjF-XEF4Ac9",
        "outputId": "193a426e-8a90-46b2-b0d3-dbc13e66ccbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 1]),\n",
              " torch.Size([32, 27]),\n",
              " tensor([1.8626e-09]),\n",
              " tensor(1.8626e-09, grad_fn=<SumBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dlogits\n",
        "logits are used twice in forward pass, once in norm_logits, once in logit_maxes.\n",
        "\n",
        "For norm_logits:\n",
        "$$\\text{norm_logits}=logits - \\text{logit_maxes}\n",
        "$$\n",
        "$$\\dfrac{\\delta \\text{norm_logits}}{\\delta logits} = 1\n",
        "$$\n",
        "\n",
        "For logit_maxes:\n",
        "$$\\text{logit_maxes}=max(logits_j)  \\quad \\text{for }j \\in [0, \\text{vocab_size})\n",
        "$$\n",
        "Therefore, for each row, only the max value index contributes to the gradient, the rest of the gradient is zero.\n",
        "$$\\dfrac{\\delta \\text{logit_maxes}}{\\delta logits}=\n",
        "\\begin{cases}\n",
        "1 & \\quad \\text{when }logits_j \\text{ is the max of the sample}\\\\\n",
        "0 & \\quad \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "6MUADJr1xu8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits1 = dnorm_logits\n",
        "dlogits2 = torch.zeros_like(logits)\n",
        "dlogits2[range(logits.shape[0]), torch.argmax(logits, dim=1)]=1\n",
        "dlogits2 *= dlogit_maxes\n",
        "\n",
        "dlogits = dlogits1 + dlogits2\n",
        "cmp('dlogits', dlogits, logits)"
      ],
      "metadata": {
        "id": "rUlyIz-Hx1eK",
        "outputId": "e5d26241-61eb-40f8-f7a1-92bc56434334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dlogits         | exact: True  | approx: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2 - backward of output layer"
      ],
      "metadata": {
        "id": "QcBnehFN-93U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "logits = h @ W2 + b2\n",
        "\n",
        "manipulate to match the dimensions."
      ],
      "metadata": {
        "id": "ur-q2bec_Igd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W2.shape, dlogits.shape, h.shape, b2.shape"
      ],
      "metadata": {
        "id": "tawsmCSC_jRe",
        "outputId": "116770c9-2902-4c9f-e8b4-b1158c4a3384",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([200, 27]),\n",
              " torch.Size([32, 27]),\n",
              " torch.Size([32, 200]),\n",
              " torch.Size([27]))"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dW2 = h.T @ dlogits\n",
        "cmp('dW2', dW2, W2)"
      ],
      "metadata": {
        "id": "uqihgwvy_BjP",
        "outputId": "e5640a60-72dd-4d26-ffcd-4b30a5219212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dW2             | exact: True  | approx: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db2 = torch.ones_like(b2) * dlogits\n",
        "db2 = db2.sum(dim=0) # db2 dim is (vocab_size), so it contributes 1 per column\n",
        "cmp('db2', db2, b2)"
      ],
      "metadata": {
        "id": "8lFEMby-Anw1",
        "outputId": "027698d7-6218-4ada-b857-d43a50db87f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "db2             | exact: True  | approx: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dh = dlogits @ W2.T\n",
        "cmp('dh', dh, h)"
      ],
      "metadata": {
        "id": "gm2zyks5DHoI",
        "outputId": "d5c62f0a-e996-41ab-d920-ef70293f1c06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dh              | exact: True  | approx: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3 - backward of activation\n",
        "h = torch.tanh(h_preact)\n",
        "\n",
        "gradient of tanh(x) is 1-tanh(x)**2"
      ],
      "metadata": {
        "id": "c6mqXwa1Cozj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dh_preact = (1-torch.square(h)) * dh\n",
        "#dh_preact *= dh\n",
        "cmp('dh_preact', dh_preact, h_preact)"
      ],
      "metadata": {
        "id": "mDfxAkJdB6vF",
        "outputId": "7641fbc1-d6a5-4e66-8f14-4f513ebf9219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dh_preact       | exact: False | approx: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4 - backward of Batch Norm layer"
      ],
      "metadata": {
        "id": "cXCha0D4Bg1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gamma, beta, x_hat"
      ],
      "metadata": {
        "id": "Bg3wclMqBrTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bn_gamma.shape, h_preact.shape, bn_bias.shape, x_hat.shape"
      ],
      "metadata": {
        "id": "ulzeOclHEmrt",
        "outputId": "d20daf6a-1469-4c3e-9ce4-1c60f6dee4e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 200]),\n",
              " torch.Size([32, 200]),\n",
              " torch.Size([1, 200]),\n",
              " torch.Size([32, 200]))"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbn_gamma"
      ],
      "metadata": {
        "id": "WKii5_L4EjM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  bn_mean = 1/batch_size*h_prebn.sum(dim=0, keepdim=True)\n",
        "  #bn_std = h_prebn.std(dim=0, keepdim=True)\n",
        "  bn_diff = (h_prebn - bn_mean)\n",
        "  bn_diff2 = bn_diff ** 2\n",
        "  bn_var = 1/(batch_size-1) * bn_diff.sum(dim=0, keepdim=True) # Bessel's correction, divide by m-1 not m\n",
        "  bn_varinv = (bn_var + 1e-5)**-0.5  # 1/sqrt(var+eps)\n",
        "  x_hat = bn_diff * bn_varinv\n",
        "  h_preact = bn_gamma * x_hat + bn_bias\n"
      ],
      "metadata": {
        "id": "ezz7DoBMBo79"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "makemore_backprop.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}